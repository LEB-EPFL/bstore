{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple datastore from a small experiment\n",
    "In this first tutorial we'll take a small experiment which includes raw localizations, widefield images, and metadata and build them into a database. The purpose of doing so is to provide a compact, well-organized representation of single molecule localization microscopy (SMLM) data which faciltaties high content analysis and reproducibility.\n",
    "\n",
    "The datastore will exist within an [HDF](https://www.hdfgroup.org/) file. The organization of the data inside the file will be handled by B-Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the essential bstore libraries\n",
    "from bstore import database, parsers\n",
    "\n",
    "# This is part of Python 3.4 and greater and not part of B-Store\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting: Get the test data\n",
    "You can get the test data for this tutorial from the B-Store test repository at https://github.com/kmdouglass/bstore_test_files. Clone or download the files and change the filename below to point to the folder *parsers_test_files/SimpleParser* within this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchDirectory = Path('../../bstore_test_files/parsers_test_files/SimpleParser/') # ../ means go up one directory level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SimpleParser* contains a few folders containing data from an imaging experiment performed on HeLa cells using STORM. The raw STORM localizations are in the files matching the pattern \\*.csv. Metadata for the localizations are in JSON format and stored in files matching the \\*.txt pattern. Before each STORM image, widefield images were captured and saved in an TIFF format. The naming patterns for these files is \\*.tif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step one: Create a parser to read the datasets\n",
    "In this step, we'll create a parser that can read the files that are stored inside the test data directory and convert them into a format that's more suitable for automated organization and retrieval. One of the default parsers that comes with B-Store and that we'll use in this tutorial is called `SimpleParser`. This parser transforms filenames of the format *prefix_acqID.fileExtension* into DatasetIDs. *prefix* is a descriptive name given to a dataset, such as *HeLa_Cells* or *treatment* and *acqID* is an integer uniquely identifying the field of view.\n",
    "\n",
    "Because every lab acquires and computes localizations differently, you can use a more customizable parser known as the `PositionParser`, or even write your own in Python code and store it in B-Store's plugins directory: `~/.bstore/bsplugins`. (Note that on Windows `~` becomes `%USERPROFILE%`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the parser\n",
    "parser = parsers.SimpleParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Of course, this step is easy if a parser already exists for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step two: Create the empty datastore object\n",
    "The `HDFDatastore` object is what B-Store uses to build a datastore inside an HDF file. All `HDFDatastore` objects are essentially sets of `DatasetID`s, with some additional functionality to make it easy to get and put data from and into the datastore. The `Parser`'s job is to assign unique `DatasetID`s to your files based on their naming pattern.\n",
    "\n",
    "This type of design feature, where data and metadata is structured in a certain way as it passes into and out of a database, is known as an interface. The advantage is that you can structure your data however you want on either side of the interface so long as it can be translated into the right format. The format is defined by the `DatasetID` object named `HDFDatastore.dsID`.\n",
    "\n",
    "When we create the datastore, we specify a path to the file where the file will be stored. Note that no file is created until data is actually put into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The path is relative to this notebook.\n",
    "# Altnernatively, you could send a Path object\n",
    "# instead of a string to HDFDatabase constructor.\n",
    "dsName = 'myFirstDatastore.h5'\n",
    "myDS   = database.HDFDatastore(dsName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step three: Run a test build of the datastore\n",
    "Now comes the fun part. We build the datastore by using the HDFDatastore's `build()` method. To do this, we need to send a few required arguments to the method. These are:\n",
    "\n",
    "1. `parser` - The parser we created to interpret the data files\n",
    "2. `searchDirectory` - The parent directory containing files and subdirectories with all the experimental data\n",
    "\n",
    "We also need to specify each type of data we want to include. First we register the types of datasets we want to work with like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bstore.config\n",
    "bstore.config.__Registered_DatasetTypes__ = ['Localizations', 'LocMetadata', 'WidefieldImage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Localizations`, `LocMetadata`, and `WidefieldImage` are three built-in types of datasets. A list of dataset types and their code may be found here: https://github.com/kmdouglass/bstore/tree/master/bstore/datasetTypes\n",
    "\n",
    "Once the dataset types are registered, we need to tell the build process what files correspond to what dataset types. To do this we will pass a dict called filenameStrings to the `build()` method.\n",
    "\n",
    "```\n",
    "filenameStrings = {'Localizations' :  '.csv',\n",
    "                   'LocMetadata'   :  '.txt',\n",
    "                   'WidefieldImage' : '.tif'}\n",
    "```\n",
    "\n",
    "In this example, localizations are saved in .csv files. If there are special naming patterns to your files, you can use wildcards to identify your files. For example, if your localization files follow the pattern **prefix**\\_locs\\_**acqID**.csv, then you can pass locs\\*.csv instead of .csv above to better specify the files.\n",
    "\n",
    "Finally, there is a boolean argument named `dryRun`. If you set this to True, the build method won't actually create the database. It will however return a structure that tells you what datasets were successfully parsed and capable of insertion into the database. By default, `dryRun` is set to False.\n",
    "\n",
    "Let's go ahead and set `build()`'s arguments and do a dry run of the build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 files were successfully parsed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>datasetType</th>\n",
       "      <th>attributeOf</th>\n",
       "      <th>channelID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>posID</th>\n",
       "      <th>sliceID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th>acqID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">HeLaL_Control</th>\n",
       "      <th>1</th>\n",
       "      <td>WidefieldImage</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LocMetadata</td>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">HeLaS_Control</th>\n",
       "      <th>2</th>\n",
       "      <td>WidefieldImage</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LocMetadata</td>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        datasetType    attributeOf channelID dateID posID  \\\n",
       "prefix        acqID                                                         \n",
       "HeLaL_Control 1      WidefieldImage           None      None   None  None   \n",
       "              1       Localizations           None      None   None  None   \n",
       "              1         LocMetadata  Localizations      None   None  None   \n",
       "HeLaS_Control 2      WidefieldImage           None      None   None  None   \n",
       "              2       Localizations           None      None   None  None   \n",
       "              2         LocMetadata  Localizations      None   None  None   \n",
       "\n",
       "                    sliceID  \n",
       "prefix        acqID          \n",
       "HeLaL_Control 1        None  \n",
       "              1        None  \n",
       "              1        None  \n",
       "HeLaS_Control 2        None  \n",
       "              2        None  \n",
       "              2        None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the default values for locMetadataString and widefieldImageString\n",
    "# will work in this example\n",
    "myDS.build(parser, searchDirectory,\n",
    "           filenameStrings = {'Localizations' :  '.csv',\n",
    "                              'LocMetadata'   :  '.txt',\n",
    "                              'WidefieldImage' : '.tif'},\n",
    "           dryRun = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table contains all the datasets that the `HDFDatastore` found in the `searchDirectory` and is sorted by the acqusition's prefix and ID number. Let's go through these results to understand what they are telling us.\n",
    "\n",
    "## prefix and acqID\n",
    "The `prefix` is the descriptive name given to a dataset. In this example, it contains the cell type (HeLaS) and the conditions (Control). The prefix can be anything you want and is required for insertion into the database. The table is telling us that two different conditions were imaged, and for these conditions there was one acquisition.\n",
    "\n",
    "The `acqID` number is an integer that identifies an acquisition and is also required. An acquisition is simply a collection of datasets containing, for example, localizations, metadata, and possibly widefield images of a single field of view. The set of all acquisitions with the same `prefix` form an acqusition group.\n",
    "\n",
    "You can also see that the acqID need not start at one, since the first acqID in the HeLaS_Control group is 2.\n",
    "\n",
    "## datasetType\n",
    "The `datasetType` is also a required ID. The `datasetType` tells the datastore what type of data it is looking at during the build operation so that it knows how to store it.\n",
    "\n",
    "Currently, `datasetType` supports three options:\n",
    "\n",
    "1. Localizations - Tabulated localization data in a raw text format (can be comma separated, tab-separated, etc.)\n",
    "2. LocMetadata - Textual metadata describing the localizations (currently only JSON is supported)\n",
    "3. WidefieldImage - A single widefield image of the field of view (.tif and .OME.TIFF is supported)\n",
    "4. FiducialTracks - Tabulated raw text data on localizations from fiducial markers\n",
    "5. AverageFiducial - An average over many fiducial tracks, also in tabulated form\n",
    "\n",
    "## attributeOf\n",
    "\n",
    "Datasets that describe other datasets have an `attributeOf` field. Because `LocMetadata` describes `Localizations`, you can see that `Localizations` is listed in the corresponding entry.\n",
    "\n",
    "## channelID, dateID, posID, and sliceID\n",
    "These fields are optional and specify the fluorescence channel, date of the acqusition, position, and axial slice of a field of view, respectively. They serve to more precisely identify datasets in complex acquisitions.\n",
    "\n",
    "The channel can be any string you want, such as `A647`.\n",
    "\n",
    "The dateID is given in a format like YYYY-MM-DD.\n",
    "\n",
    "The position ID usually follows a format like `(0,)`, which is a single integer identifying the position corresponding to this dataset. This allows the user to specify different positions on a sample that were imaged within the same acquisition. It can also take the form of a two-element tuple like `(x,y)` if desired.\n",
    "\n",
    "The slice ID is simply an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the real database build\n",
    "Now that we've verified everything going into the database, we can build it by detting `dryRun` to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 files were successfully parsed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>datasetType</th>\n",
       "      <th>attributeOf</th>\n",
       "      <th>channelID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>posID</th>\n",
       "      <th>sliceID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th>acqID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">HeLaL_Control</th>\n",
       "      <th>1</th>\n",
       "      <td>WidefieldImage</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LocMetadata</td>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">HeLaS_Control</th>\n",
       "      <th>2</th>\n",
       "      <td>WidefieldImage</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LocMetadata</td>\n",
       "      <td>Localizations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        datasetType    attributeOf channelID dateID posID  \\\n",
       "prefix        acqID                                                         \n",
       "HeLaL_Control 1      WidefieldImage           None      None   None  None   \n",
       "              1       Localizations           None      None   None  None   \n",
       "              1         LocMetadata  Localizations      None   None  None   \n",
       "HeLaS_Control 2      WidefieldImage           None      None   None  None   \n",
       "              2       Localizations           None      None   None  None   \n",
       "              2         LocMetadata  Localizations      None   None  None   \n",
       "\n",
       "                    sliceID  \n",
       "prefix        acqID          \n",
       "HeLaL_Control 1        None  \n",
       "              1        None  \n",
       "              1        None  \n",
       "HeLaS_Control 2        None  \n",
       "              2        None  \n",
       "              2        None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDS.build(parser, searchDirectory,\n",
    "           filenameStrings = {'Localizations' :  '.csv',\n",
    "                              'LocMetadata'   :  '.txt',\n",
    "                              'WidefieldImage' : '.tif'},\n",
    "           dryRun = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the HDF file was created in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('./myFirstDatastore.h5').exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling data from the database\n",
    "Now that data has been placed inside our database, how do we get it out?\n",
    "\n",
    "We can use the `HDFDatastore.get()` method to get the data for a specific dataset. The `get()` method accepts a DatasetID (`HDFDatastore.dsID`) that specifies the dataset's ID's and returns an object allowing access to the data.\n",
    "\n",
    "The order of IDs is important; it is:\n",
    "\n",
    "1. prefix\n",
    "2. acqID\n",
    "3. datasetType\n",
    "4. attributeOf\n",
    "5. channelID\n",
    "6. dateID\n",
    "7. posID\n",
    "8. sliceID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the dataset ID's\n",
    "dsID = myDS.dsID('HeLaL_Control', 1, 'Localizations', None, None, None, None, None)\n",
    "\n",
    "# Extract the dataset from the database\n",
    "myData = myDS.get(dsID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can access myData's `data` field to actually access the data in the database. Here, we compute some summary statistics and display the first few rows of the localization data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>intensity</th>\n",
       "      <th>offset</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8994.581818</td>\n",
       "      <td>59467.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.993009</td>\n",
       "      <td>10992.20000</td>\n",
       "      <td>720.831818</td>\n",
       "      <td>1847.315455</td>\n",
       "      <td>179.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1170.696295</td>\n",
       "      <td>1687.184034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.013617</td>\n",
       "      <td>8734.24533</td>\n",
       "      <td>367.812667</td>\n",
       "      <td>3631.486533</td>\n",
       "      <td>39.753501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6770.000000</td>\n",
       "      <td>56713.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.078700</td>\n",
       "      <td>3107.80000</td>\n",
       "      <td>270.240000</td>\n",
       "      <td>243.080000</td>\n",
       "      <td>111.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8024.150000</td>\n",
       "      <td>58228.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.314400</td>\n",
       "      <td>7599.90000</td>\n",
       "      <td>508.740000</td>\n",
       "      <td>554.720000</td>\n",
       "      <td>158.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9163.200000</td>\n",
       "      <td>59647.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.507200</td>\n",
       "      <td>8408.10000</td>\n",
       "      <td>641.580000</td>\n",
       "      <td>643.070000</td>\n",
       "      <td>198.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9866.600000</td>\n",
       "      <td>60286.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.180550</td>\n",
       "      <td>11132.60000</td>\n",
       "      <td>922.995000</td>\n",
       "      <td>1064.220000</td>\n",
       "      <td>201.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10350.000000</td>\n",
       "      <td>62858.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>35038.00000</td>\n",
       "      <td>1346.000000</td>\n",
       "      <td>12727.000000</td>\n",
       "      <td>218.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y     z  frame  uncertainty    intensity  \\\n",
       "count     11.000000     11.000000  11.0   11.0    11.000000     11.00000   \n",
       "mean    8994.581818  59467.181818   0.0   50.0     5.993009  10992.20000   \n",
       "std     1170.696295   1687.184034   0.0    0.0     3.013617   8734.24533   \n",
       "min     6770.000000  56713.000000   0.0   50.0     1.078700   3107.80000   \n",
       "25%     8024.150000  58228.500000   0.0   50.0     4.314400   7599.90000   \n",
       "50%     9163.200000  59647.000000   0.0   50.0     6.507200   8408.10000   \n",
       "75%     9866.600000  60286.000000   0.0   50.0     7.180550  11132.60000   \n",
       "max    10350.000000  62858.000000   0.0   50.0    10.883000  35038.00000   \n",
       "\n",
       "            offset  loglikelihood       sigma  \n",
       "count    11.000000      11.000000   11.000000  \n",
       "mean    720.831818    1847.315455  179.280000  \n",
       "std     367.812667    3631.486533   39.753501  \n",
       "min     270.240000     243.080000  111.560000  \n",
       "25%     508.740000     554.720000  158.095000  \n",
       "50%     641.580000     643.070000  198.220000  \n",
       "75%     922.995000    1064.220000  201.995000  \n",
       "max    1346.000000   12727.000000  218.790000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe() is a Pandas DataFrame method that displays\n",
    "# summary statistics\n",
    "myData.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>intensity</th>\n",
       "      <th>offset</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6770.0</td>\n",
       "      <td>59386</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>9.5138</td>\n",
       "      <td>4386.6</td>\n",
       "      <td>270.24</td>\n",
       "      <td>425.92</td>\n",
       "      <td>218.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7958.1</td>\n",
       "      <td>59762</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>6.7329</td>\n",
       "      <td>8310.3</td>\n",
       "      <td>562.65</td>\n",
       "      <td>619.47</td>\n",
       "      <td>199.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7840.8</td>\n",
       "      <td>60819</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2.1987</td>\n",
       "      <td>15671.0</td>\n",
       "      <td>1261.10</td>\n",
       "      <td>1691.40</td>\n",
       "      <td>119.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8090.2</td>\n",
       "      <td>59801</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>7.6282</td>\n",
       "      <td>6952.3</td>\n",
       "      <td>642.53</td>\n",
       "      <td>506.19</td>\n",
       "      <td>206.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9010.3</td>\n",
       "      <td>59647</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>6.5814</td>\n",
       "      <td>8408.1</td>\n",
       "      <td>684.29</td>\n",
       "      <td>821.24</td>\n",
       "      <td>197.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x      y  z  frame  uncertainty  intensity   offset  loglikelihood  \\\n",
       "0  6770.0  59386  0     50       9.5138     4386.6   270.24         425.92   \n",
       "1  7958.1  59762  0     50       6.7329     8310.3   562.65         619.47   \n",
       "2  7840.8  60819  0     50       2.1987    15671.0  1261.10        1691.40   \n",
       "3  8090.2  59801  0     50       7.6282     6952.3   642.53         506.19   \n",
       "4  9010.3  59647  0     50       6.5814     8408.1   684.29         821.24   \n",
       "\n",
       "    sigma  \n",
       "0  218.79  \n",
       "1  199.50  \n",
       "2  119.47  \n",
       "3  206.46  \n",
       "4  197.90  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head() is a Pandas DataFrame method that displays\n",
    "# the first five rows\n",
    "myData.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-like operations on HDFDatastores\n",
    "\n",
    "HDFDatastores support many standard Python operations for sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of datasets\n",
    "len(myDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetID(prefix='HeLaL_Control', acqID=1, datasetType='WidefieldImage', attributeOf=None, channelID=None, dateID=None, posID=None, sliceID=None)\n",
      "DatasetID(prefix='HeLaS_Control', acqID=2, datasetType='WidefieldImage', attributeOf=None, channelID=None, dateID=None, posID=None, sliceID=None)\n",
      "DatasetID(prefix='HeLaL_Control', acqID=1, datasetType='Localizations', attributeOf=None, channelID=None, dateID=None, posID=None, sliceID=None)\n",
      "DatasetID(prefix='HeLaS_Control', acqID=2, datasetType='Localizations', attributeOf=None, channelID=None, dateID=None, posID=None, sliceID=None)\n",
      "DatasetID(prefix='HeLaL_Control', acqID=1, datasetType='LocMetadata', attributeOf='Localizations', channelID=None, dateID=None, posID=None, sliceID=None)\n",
      "DatasetID(prefix='HeLaS_Control', acqID=2, datasetType='LocMetadata', attributeOf='Localizations', channelID=None, dateID=None, posID=None, sliceID=None)\n"
     ]
    }
   ],
   "source": [
    "# Iteration\n",
    "for ds in myDS:\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetID(prefix='HeLaL_Control', acqID=1, datasetType='WidefieldImage', attributeOf=None, channelID=None, dateID=None, posID=None, sliceID=None)]\n"
     ]
    }
   ],
   "source": [
    "# Filtering and list comprehensions\n",
    "filteredSets = [ds for ds in myDS if ds.prefix == 'HeLaL_Control' and ds.datasetType == 'WidefieldImage']\n",
    "\n",
    "print(filteredSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetID(prefix='HeLaL_Control', acqID=1, datasetType='Localizations', attributeOf=None, channelID=None, dateID=None, posID=None, sliceID=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integer-based indexing\n",
    "myDS[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "1. A B-Store datastore is an organized collection of raw data and metadata from an SMLM experiment.\n",
    "2. B-Store provides a built-in datastore known as `HDFDatastore` that stores the data in an HDF file.\n",
    "3. A datastore requires a `Parser` to convert your files into the format that the datastore knows how to handle.\n",
    "4. B-Store organizes datasets into acquisition groups that are defined by a **prefix** and **acquisition ID**. A single acquisition is defined by a **datasetType** and possibly a **channel ID**, **dateID**, **position ID**, and a **slice ID**.\n",
    "5. You can perform a dry run before building to verify what files will go into the database using `build(dryRun = True)`.\n",
    "6. After building the datastore, data may be retrieved using the `get()` method.\n",
    "7. HDFDatastores support many standard Python operations for sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete the database example file\n",
    "import os\n",
    "os.remove('myFirstDatastore.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
