{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing custom parsers\n",
    "B-Store was designed to work with your data by not enforcing strict rules about file formats. This means, for example, that you are not required to follow a certain column naming convention or to use .csv files when generating your raw data.\n",
    "\n",
    "While this gives you a lot of flexibility when acquiring your data in the lab, it does come at a cost: you must write your own parser to translate your files into a format that can be organized by B-Store.\n",
    "\n",
    "B-Store comes with a built-in parser known as a `SimpleParser` to provide out-of-the-box functionality for simple datasets. In this tutorial, we'll write the SimpleParser from scratch to demonstrate how you may write your own parsers for B-Store.\n",
    "\n",
    "## The logic of B-Store\n",
    "B-Store was designed to take localization data, widefield images, and metadata and convert them into a format that is easily stored for both human and machine interpretation. This logic is illustrated below:\n",
    "\n",
    "<img src=\"../design/dataset_logic.png\" width = 50%/>\n",
    "\n",
    "The role of the `Parser` is take these raw datasets and assign to them a descriptive name (known as a `prefix`) that identifies datasets that should be grouped together, such as grouping data from controls and treatments into separate groups. Within these groups, which are known as acquisition groups, each dataset is identified by a number known as the `acqID` and the type of data it contains, the `datasetType`. Finally, there are a number of other fields that may identify the dataset if more precise delimitation between datasets is required.\n",
    "\n",
    "When provided with a file, a `Parser` is required to specify the following fields:\n",
    "\n",
    "- `acqID` - a unique integer for a given prefix\n",
    "- `prefix` - a string that gives a descriptive name to the dataset\n",
    "- `datasetType` - one of the strings listed in the `__Types_Of_Atoms__` variable in *config.py*; at the time of writing, these are 'locResults', 'locMetadata', or 'widefieldImage'\n",
    "\n",
    "Additionally, the `Parser` must provide a way to access the actual data contained in a file. Depending on the `datasetType`, the data from a file is represented internally as one of these data types after loading from memory:\n",
    "\n",
    "- `locResults` - [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "- `locMetadata` - [JSON](http://www.json.org/) string-value pairs\n",
    "- `widefieldImage` - 2D [Numpy](http://www.numpy.org/) array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Parser` interface\n",
    "The reason that B-Store needs this ID information is that organization in the database can be automated only if the data matches the database interface. In B-Store, this interface is known as a `DatabaseAtom`; an actual realization of a dataset is known as a `Dataset`. In software engineering terms, the `Dataset` class *implements* the `DatabaseAtom` interface, which just means that a `Dataset` knows how to communicate with a database and vice versa.\n",
    "\n",
    "To ease its creation, a parser must also implement an interface known as a `Parser`. The `Parser` interface is simply a list of functions that a Python class must implement to be called a `Parser`. Let's start by looking at the code for this interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import B-Store's parsers module\n",
    "from bstore import parsers\n",
    "\n",
    "# Used to retrieve the code\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Parser(metaclass = ABCMeta):\n",
      "    \"\"\"Translates files to machine-readable data structures with acq. info.\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    acqID       : int\n",
      "        The number identifying the Multi-D acquisition for a given prefix name.\n",
      "    channelID   : str\n",
      "        The color channel associated with the dataset.\n",
      "    dateID      : str\n",
      "        The date of the acquistion in the format YYYY-mm-dd.\n",
      "    posID       : (int,) or (int, int)\n",
      "        The position identifier. It is a single element tuple if positions were\n",
      "        manually set; otherwise, it's a 2-tuple indicating the x and y\n",
      "        identifiers.\n",
      "    prefix      : str\n",
      "        The descriptive name given to the dataset by the user.\n",
      "    sliceID     : int\n",
      "        The number identifying the z-axis slice of the dataset.\n",
      "    datasetType : str\n",
      "        The type of data contained in the dataset. Can be one of 'locResults',\n",
      "        'locMetadata', or 'widefieldImage'.\n",
      "       \n",
      "    \"\"\"\n",
      "    def __init__(self, prefix, acqID, datasetType,\n",
      "                 channelID = None, dateID = None,\n",
      "                 posID = None, sliceID = None):\n",
      "        \"\"\"Initialize the parser's metadata information.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        acqID       : int\n",
      "            The number identifying the Multi-D acquisition for a given prefix\n",
      "            name.\n",
      "        channelID   : str\n",
      "            The color channel associated with the dataset.\n",
      "        dateID      : str\n",
      "            The date of the acquistion in the format YYYY-mm-dd.\n",
      "        posID       : int, or (int, int)\n",
      "            The position identifier. It is a single element tuple if positions\n",
      "            were manually set; otherwise, it's a 2-tuple indicating the x and y\n",
      "            identifiers.\n",
      "        prefix      : str\n",
      "            The descriptive name given to the dataset by the user.\n",
      "        sliceID     : int\n",
      "            The number identifying the z-axis slice of the dataset.\n",
      "        datasetType : str\n",
      "            The type of data contained in the dataset. Can be one of\n",
      "            'locResults', 'locMetadata', or 'widefieldImage'.\n",
      "        \n",
      "        \"\"\"\n",
      "        if datasetType not in database.typesOfAtoms:\n",
      "            raise DatasetError(datasetType)\n",
      "        \n",
      "        # These are the essential pieces of information to identify a dataset.\n",
      "        self.acqID       =       acqID\n",
      "        self.channelID   =   channelID\n",
      "        self.dateID      =      dateID\n",
      "        self.posID       =       posID\n",
      "        self.prefix      =      prefix\n",
      "        self.sliceID     =     sliceID\n",
      "        self.datasetType = datasetType\n",
      "        \n",
      "    def getBasicInfo(self):\n",
      "        \"\"\"Return a dictionary containing the basic dataset information.\n",
      "        \n",
      "        \"\"\"\n",
      "        basicInfo = {\n",
      "                     'acqID'         :       self.acqID,\n",
      "                     'channelID'     :   self.channelID,\n",
      "                     'dateID'        :      self.dateID,\n",
      "                     'posID'         :       self.posID,\n",
      "                     'prefix'        :      self.prefix,\n",
      "                     'sliceID'       :     self.sliceID,\n",
      "                     'datasetType'   : self.datasetType\n",
      "                     }\n",
      "                     \n",
      "        return basicInfo\n",
      "    \n",
      "    @property\n",
      "    def prefix(self):\n",
      "        return self._prefix\n",
      "        \n",
      "    @prefix.setter\n",
      "    def prefix(self, value):\n",
      "        if value:\n",
      "            # Replaces spaces with '_' in prefix.\n",
      "            # This avoids problems with spaces in PyTables\n",
      "            self._prefix = value.replace(' ', '_')\n",
      "    \n",
      "    @abstractproperty\n",
      "    def data(self):\n",
      "        \"\"\"Loads the data into memory and maps it to the correct format.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "    \n",
      "    @abstractmethod\n",
      "    def getDatabaseAtom(self):\n",
      "        \"\"\"Returns one atomic unit for insertion into the Database.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "    \n",
      "    @abstractmethod\n",
      "    def parseFilename(self):\n",
      "        \"\"\"Parses a file for conversion to a DatabaseAtom.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(parsers.Parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the code above, we can see that a `Parser` has two functions:\n",
    "\n",
    "- `__init__()` : the constructor that assigns the class fields\n",
    "- `getBasicInfo()` : returns a dictionary with the Parser's information\n",
    "\n",
    "Furthermore, there are a few functions that are preceded by `abstractproperty` or `abstractmethod` that don't actually do anything (their body's contents only contain the word `pass`). These are the functions and properties that our custom `Parser` must define to work with our data. They are:\n",
    "\n",
    "- `data` - contains the actual data from a file\n",
    "- `getDatabaseAtom()` - returns a DatabaseAtom instance that can be put inside a B-Store database\n",
    "- `parseFilename` - generates the DatabaseAtom ID fields from a file or filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the `SimpleParser`\n",
    "\n",
    "## File naming conventions\n",
    "For the sake of this tutorial, let's suppose that our acquisition software produces files that follow this naming convention:\n",
    "\n",
    "- **prefix_acqID.csv** : `locResults` come in .csv files that with a common name, followed by an underscore, and then an integer identifier. For example, HeLa_2.csv\n",
    "- **prefix_acqID.txt** : `locMetadata` is found in .txt files with prefixes and acquisition ID's that match their corresponding localization data\n",
    "- **prefix_acqID.tif** : `widefieldImage`'s are found in tif files that also match the corresponding the localization data.\n",
    "\n",
    "## SimpleParser inputs and outputs\n",
    "Our `SimpleParser` will be relatively, well, simple to convert these files into a format that B-Store can organize. This will hopefully give you the main idea about how you may write your own and provide a base class for doing so.\n",
    "\n",
    "The parser's constructor will take no arguments. It's main function, `parseFilename()` will take a string as input that represents a file's name and another string representing the `datasetType` of the file. This function will set the ID fields of the `Parser` and also tell the Parser how to read the data.\n",
    "\n",
    "Let's write an outline of this class following this design that doesn't actually do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "class SimpleParser(Parser):\n",
    "    \"\"\"A simple parser for extracting acquisition information.\n",
    "    \n",
    "    The SimpleParser converts files of the format prefix_acqID.* into\n",
    "    DatabaseAtoms for insertion into a database. * may represent .csv files\n",
    "    (for locResults), .json (for locMetadata), and .tif (for widefieldImages).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getDatabaseAtom(self):\n",
    "        pass\n",
    "    \n",
    "    def parseFilename(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        pass \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the skeleton above we have all the functions and the `data` property that are required by the interface, plus a constructor named `__init__()`. The problem is, there's no actual functionality at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `parseFilename()`\n",
    "Most of the work done by the Parser is the `parseFilename()` function. This function reads a filename and then fills in the appropriate fields of `Parser` parent class, like `acqID`, `prefix`, etc. The function should also take an argument that we'll call `datasetType` that tells it what kind of dataset it's looking at. The function then handles each type of dataset differently.\n",
    "\n",
    "Let's add this argument and another named `filename`, then begin to flesh out the function.\n",
    "\n",
    "```python\n",
    "def parseFilename(self, filename, datasetType = 'locResults'):\n",
    "    \"\"\"Converts a filename into a DatabaseAtom.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        A string or pathlib Path object containing the dataset's filename.\n",
    "    dsType   : str\n",
    "        The type of the dataset being parsed. This tells the Parser\n",
    "        how to interpret the data.\n",
    "            \n",
    "    \"\"\"\n",
    "    pass # Don't do anything yet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll save the filename which contains the full path to the file to a private variable for later use.\n",
    "\n",
    "```python\n",
    "# Save the full path to the file for later.\n",
    "# If filename is already a Path object, this does nothing.\n",
    "self._fullPath = pathlib.Path(filename) \n",
    "```\n",
    "\n",
    "Next, we need to account for the fact that the input filename can be either a string or a pathlib `Path` object. To do this, we convert a `Path` to a string using the `str()` function. This is done because we'll use string manipulations later to parse the filename.\n",
    "\n",
    "```python\n",
    "# Convert Path objects to strings if Path is supplied\n",
    "if isinstance(filename, pathlib.PurePath):\n",
    "    filename = str(filename.name)\n",
    "```\n",
    "\n",
    "We use `Path`'s parent, a `PurePath` object, because its output is the same regardless of the user's operating system. The `.name` property of a path is simply the file's name without the parent folders.\n",
    "\n",
    "Now let's look again briefly at the naming convention of our data. All of our files follow the rule **prefix_acqID.xxx**. This means that the file type--.csv, .txt, or .tif--already tells us the dataset type. The first part of the filename will always tell us the `prefix`, which can be anything, and the last part will always be an underscore followed by an integer `acqID`.\n",
    "\n",
    "We can easily extract this information with Python's built-in string manipulation tools and the *os.path* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove the file type: path/to/HeLa_Control_7\n",
      "Remove the file type and parent folders: HeLa_Control_7\n",
      "HeLa_Control_7\n"
     ]
    }
   ],
   "source": [
    "from os.path import splitext\n",
    "\n",
    "# Example\n",
    "filename = 'path/to/HeLa_Control_7.csv'\n",
    "\n",
    "# Remove the '.csv'\n",
    "print('Remove the file type: ' + splitext(filename)[0])\n",
    "\n",
    "# Remove any parent folders\n",
    "print('Remove the file type and parent folders: ' + splitext(filename)[0].split('/')[-1])\n",
    "\n",
    "# This works if there are no parents folders, too\n",
    "print(splitext('HeLa_Control_7.csv')[0].split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prefix` and `acqID` values are easy to get. We simply split the string at the last underscore and take the part before it as the `prefix` and the part after as the `acqID`. Python's `rsplit()` function does this for us. Finally, we convert the `acqID` from a string to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix is: HeLa_Control\n",
      "acqID is: 7\n"
     ]
    }
   ],
   "source": [
    "# Isolate the root filename\n",
    "rootName = splitext(filename)[0].split('/')[-1]\n",
    "\n",
    "# Split the string at the last underscore\n",
    "prefix, acqID = rootName.rsplit('_', 1)\n",
    "acqID = int(acqID) # Convert the string to an integer\n",
    "\n",
    "print('prefix is: {:s}'.format(prefix))\n",
    "print('acqID is: {:d}'.format(acqID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datasetType` was already an input to the `parseFilename()` function, so we don't need to do anything to get it from the filename. We will however add one additional part to the code to check whether the input datasetType is actually one that is recognized by B-Store. We do this by checking whether the string is inside a list of valid types called `typesOfAtoms` that is a property of the B-Store database.\n",
    "\n",
    "```python\n",
    "if datasetType not in database.typesOfAtoms:\n",
    "    raise DatasetError(datasetType)\n",
    "```\n",
    "\n",
    "Now we have all of the ID's that parser is designed to interpret: `prefix`, `acqID`, and `datasetType`. The other ID's, which are `channelID`, `dateID`, `posID`, and `sliceID`, are optional and can be implemented in your own parser. The SimpleParser will not assign values to them.\n",
    "\n",
    "We finish the function by calling the constructor of the parent of `SimpleParser`, which is known as `Parser`. This will properly assign the values extracted by the filename. Finally, we set the class field `_initialized` to True. `_initialized` will appear later in the constructor defintion, `__init__()`.\n",
    "\n",
    "```python\n",
    "super(SimpleParser, self).__init__(prefix, acqID, datasetType)\n",
    "self._initialized = True\n",
    "```\n",
    "\n",
    "The full `parseFilename` function for `SimpleParser` looks like what follows below. The whole code block is wrapped inside a try...except statement in case an error is raised during parsing. If an error is raised, the `self._initialized` field is set to False.\n",
    "\n",
    "```python\n",
    "    def parseFilename(self, filename, datasetType = 'locResults'):\n",
    "        \"\"\"Converts a filename into a DatabaseAtom.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename      : str or Path\n",
    "            A string or pathlib Path object containing the dataset's filename.\n",
    "        datasetType   : str\n",
    "            The type of the dataset being parsed. This tells the Parser\n",
    "            how to interpret the data.\n",
    "            \n",
    "        \"\"\"\n",
    "        # Check for a valid datasetType\n",
    "        if datasetType not in database.typesOfAtoms:\n",
    "            raise DatasetError(datasetType)        \n",
    "        \n",
    "        try:\n",
    "            # Save the full path to the file for later.\n",
    "            # If filename is already a Path object, this does nothing.\n",
    "            self._fullPath = pathlib.Path(filename)        \n",
    "\n",
    "            # Convert Path objects to strings if Path is supplied\n",
    "            if isinstance(filename, pathlib.PurePath):\n",
    "                filename = str(filename.name)\n",
    "\n",
    "            # Remove file type ending and any parent folders\n",
    "            # Example: 'path/to/HeLa_Control_7.csv' becomes 'HeLa_Control_7'\n",
    "            rootName = splitext(filename)[0].split('/')[-1]\n",
    "\n",
    "            # Extract the prefix and acqID\n",
    "            prefix, acqID = rootName.rsplit('_', 1)\n",
    "            acqID = int(acqID)\n",
    "\n",
    "            # Initialize the Parser\n",
    "            super(SimpleParser, self).__init__(prefix, acqID, datasetType)\n",
    "            self._initialized = True\n",
    "        except:\n",
    "            self._initialized = False\n",
    "            print('Error: File could not be parsed.', sys.exc_info()[0])\n",
    "            raise\n",
    "```\n",
    "\n",
    "#### Parsing Optional ID's\n",
    "\n",
    "If you do want to set properties like `channelID`, you can add them as optional arguments to the call to the constructor. This would look like:\n",
    "\n",
    "```python\n",
    "super(SimpleParser, self).__init__(prefix, acqID, datasetType, channelID = extractedID)\n",
    "```\n",
    "\n",
    "where `extractedID` contains whatever channel identifier you extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `data` property\n",
    "The next most important addition to our `SimpleParser` skeleton is the `data` property. This will tell the `SimpleParser` how to read the data that is in a file and format them for insertion into the database. Again, we can rely on a lot of built-in and 3rd party libraries in Python for most of this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the `data` field is accessed like a class property, it's actually defined as a function. This is achieved by inserting the [`@property` decorator](https://docs.python.org/3.5/library/functions.html#property) before the function definition.\n",
    "\n",
    "```python\n",
    "@property\n",
    "def data(self):\n",
    "    pass\n",
    "```\n",
    "\n",
    "There are currently three possible values for the `datasetType`, so this function has to define how each of these types are read. Let's setup a a series of `if...elif` statements to handle each type of dataset.\n",
    "\n",
    "```python\n",
    "def data(self):\n",
    "    if self.datasetType == 'locResults':\n",
    "        pass\n",
    "\n",
    "    elif self.datasetType == 'locMetadata':\n",
    "        pass\n",
    "\n",
    "    elif self.datasetType == 'widefieldImage':\n",
    "        pass\n",
    "        \n",
    "```\n",
    "\n",
    "### `locResults`\n",
    "Localization results are stored on disk in .csv files. In B-Store memory, however, localization results are stored in a data type known as a [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). To read the data from the .csv file into a DataFrame, we can use the `read_csv()` function supplied by Pandas.\n",
    "\n",
    "```python\n",
    "if self.datasetType == 'locResults':\n",
    "    # Loading the csv file when data() is called reduces the\n",
    "    # chance that large DataFrames do not needlessly\n",
    "    # remain in memory.\n",
    "    with open(str(self._fullPath), 'r') as file:            \n",
    "        df = pd.read_csv(file)\n",
    "        return df\n",
    "```\n",
    "\n",
    "This opens the file whose path is stored in `self._fullPath` and reads in the data using Pandas's `read_csv()`. (Note that we've assumed you have imported Pandas using `import pandas as pd` near the top of the file.) We don't require anything special to import the csv files, but if your file contains, for example, comments or a delimiter other than a comma, you can specify these in [`read_csv()`'s optional arguments](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html). Finally, the DataFrame is returned from the function\n",
    "\n",
    "### `locMetadata`\n",
    "B-Store stores metadata as [JSON strings](http://www.json.org/). We will assume that the data in our text files are already valid JSON strings and simply read them with Python's built-in `json` library. (You will need to insert the line `import json` near the top of the code file.)\n",
    "\n",
    "```python\n",
    "elif self.datasetType == 'locMetadata':\n",
    "    # Read the txt file and convert it to a JSON string.\n",
    "    with open(str(self._fullPath), 'r') as file:\n",
    "        metadata = json.load(file)\n",
    "        return metadata\n",
    "```\n",
    "\n",
    "This part is much the same as for the localization results except that we use `json.load()` to read the text file instead of `pd.read_csv()`.\n",
    "\n",
    "### `widefieldImage`\n",
    "The ability to read .tif files is provided by [matplotlib](http://matplotlib.org/), specifically the `pyplot.imread()` function. Append the line `from matplotlib.pyplot import imread` to the top of the code file to use this function.\n",
    "\n",
    "```python\n",
    "elif self.datasetType == 'widefieldImage':\n",
    "    # Load the image data only when called\n",
    "    return imread(str(self._fullPath))\n",
    "```\n",
    "\n",
    "### The full function defintion\n",
    "```python\n",
    "@property\n",
    "def data(self):\n",
    "    if self.datasetType == 'locResults':\n",
    "        # Loading the csv file when data() is called reduces the\n",
    "        # chance that large DataFrames do not needlessly\n",
    "        # remain in memory.\n",
    "        with open(str(self._fullPath), 'r') as file:            \n",
    "            df = pd.read_csv(file)\n",
    "            return df\n",
    "\n",
    "    elif self.datasetType == 'locMetadata':\n",
    "        # Read the txt file and convert it to a JSON string.\n",
    "        with open(str(self._fullPath), 'r') as file:\n",
    "            metadata = json.load(file)\n",
    "            return metadata\n",
    "\n",
    "    elif self.datasetType == 'widefieldImage':\n",
    "        # Load the image data only when called\n",
    "        return imread(str(self._fullPath))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## `getDatabaseAtom()`\n",
    "The purpose of this function is simply to return an object implementing the `DatabaseAtom` interface that is built from the parsed identifiers and data. B-Store only knows how to place objects that implement this interface into a database. (Remember that B-Store provides a `Dataset` object that already implements this interface for you.)\n",
    "\n",
    "First, we check that the parser has already been initialized by checking whether `_initialized` is False. Next, we call the superclass function `getBasicInfo()` to return a dictionary containing all the identifiers that the parser has interpreted from the file. Then, we use these identifiers to initialize a new `Dataset` and finally return it.\n",
    "\n",
    "```python\n",
    "def getDatabaseAtom(self):\n",
    "    \"\"\"Returns an object capable of insertion into a SMLM database.\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    dba : DatabaseAtom\n",
    "        One atomic unit for insertion into the database.\n",
    "\n",
    "    \"\"\"\n",
    "    if not self._initialized:\n",
    "        raise ParserNotInitializedError('Parser not initialized.')\n",
    "    \n",
    "    ids = self.getBasicInfo()\n",
    "    dba = database.Dataset(ids['prefix'], ids['acqID'], ids['datasetType'],\n",
    "                           self.data, channelID = ids['channelID'],\n",
    "                           dateID = ids['dateID'], posID = ids['posID'], \n",
    "                           sliceID = ids['sliceID'])\n",
    "    return dba\n",
    "```\n",
    "\n",
    "The above code requires placement of the import statement `from bstore import database` at the top of the code file to access the `Dataset` class. The `Dataset` constructor requires the `prefix`, `acqID`, and `datasetType` ID's and the `self.data` field in that order. The remaining ID's are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__init__()`\n",
    "The final function to define is `SimpleParser`'s constructor, which in Python is called `__init__()`. We will only need one line that sets that `_initialized` property to False.\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "   self._initialized = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `SimpleParser` class definition\n",
    "Here is the final, full definition for our `SimpleParser` class.\n",
    "\n",
    "```python\n",
    "class SimpleParser(Parser):\n",
    "    \"\"\"A simple parser for extracting acquisition information.\n",
    "    \n",
    "    The SimpleParser converts files of the format prefix_acqID.* into\n",
    "    DatabaseAtoms for insertion into a database. * may represent .csv files\n",
    "    (for locResults), .json (for locMetadata), and .tif (for widefieldImages).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._initialized = False\n",
    "    \n",
    "    def getDatabaseAtom(self):\n",
    "        \"\"\"Returns an object capable of insertion into a SMLM database.\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        dba : DatabaseAtom\n",
    "            One atomic unit for insertion into the database.\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self._initialized:\n",
    "            raise ParserNotInitializedError('Parser not initialized.')\n",
    "        \n",
    "        ids = self.getBasicInfo()\n",
    "        dba = database.Dataset(ids['prefix'], ids['acqID'], ids['datasetType'],\n",
    "                               self.data, channelID = ids['channelID'],\n",
    "                               dateID = ids['dateID'], posID = ids['posID'], \n",
    "                               sliceID = ids['sliceID'])\n",
    "        return dba\n",
    "    \n",
    "    def parseFilename(self, filename, datasetType = 'locResults'):\n",
    "        \"\"\"Converts a filename into a DatabaseAtom.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename      : str or Path\n",
    "            A string or pathlib Path object containing the dataset's filename.\n",
    "        datasetType   : str\n",
    "            The type of the dataset being parsed. This tells the Parser\n",
    "            how to interpret the data.\n",
    "            \n",
    "        \"\"\"\n",
    "        # Check for a valid datasetType\n",
    "        if datasetType not in database.typesOfAtoms:\n",
    "            raise DatasetError(datasetType)        \n",
    "        try:\n",
    "            # Save the full path to the file for later.\n",
    "            # If filename is already a Path object, this does nothing.\n",
    "            self._fullPath = pathlib.Path(filename)        \n",
    "\n",
    "            # Convert Path objects to strings if Path is supplied\n",
    "            if isinstance(filename, pathlib.PurePath):\n",
    "                filename = str(filename.name)\n",
    "\n",
    "            # Remove file type ending and any parent folders\n",
    "            # Example: 'path/to/HeLa_Control_7.csv' becomes 'HeLa_Control_7'\n",
    "            rootName = splitext(filename)[0].split('/')[-1]\n",
    "\n",
    "            # Extract the prefix and acqID\n",
    "            prefix, acqID = rootName.rsplit('_', 1)\n",
    "            acqID = int(acqID)\n",
    "\n",
    "            # Initialize the Parser\n",
    "            super(SimpleParser, self).__init__(prefix, acqID, datasetType)\n",
    "            self._initialized = True\n",
    "        except:\n",
    "            self._initialized = False\n",
    "            print('Error: File could not be parsed.', sys.exc_info()[0])\n",
    "            raise\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        if self.datasetType == 'locResults':\n",
    "            # Loading the csv file when data() is called reduces the\n",
    "            # chance that large DataFrames do not needlessly\n",
    "            # remain in memory.\n",
    "            with open(str(self._fullPath), 'r') as file:            \n",
    "                df = pd.read_csv(file)\n",
    "                return df\n",
    "                \n",
    "        elif self.datasetType == 'locMetadata':\n",
    "            # Read the txt file and convert it to a JSON string.\n",
    "            with open(str(self._fullPath), 'r') as file:\n",
    "                metadata = json.load(file)\n",
    "                return metadata\n",
    "            \n",
    "        elif self.datasetType == 'widefieldImage':\n",
    "            # Load the image data only when called\n",
    "            return imread(str(self._fullPath))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "For this example, you can use the test data in the [bstore_test_files](https://github.com/kmdouglass/bstore_test_files). Download the files from Git using the link and change the path below to point to *parsers_test_files/SimpleParsers* on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Specify the test dataset\n",
    "pathToFiles = Path('../../bstore_test_files/parsers_test_files/SimpleParser/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>intensity</th>\n",
       "      <th>offset</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8994.581818</td>\n",
       "      <td>59467.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>5.993009</td>\n",
       "      <td>10992.20000</td>\n",
       "      <td>720.831818</td>\n",
       "      <td>1847.315455</td>\n",
       "      <td>179.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1170.696295</td>\n",
       "      <td>1687.184034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.013617</td>\n",
       "      <td>8734.24533</td>\n",
       "      <td>367.812667</td>\n",
       "      <td>3631.486533</td>\n",
       "      <td>39.753501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6770.000000</td>\n",
       "      <td>56713.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.078700</td>\n",
       "      <td>3107.80000</td>\n",
       "      <td>270.240000</td>\n",
       "      <td>243.080000</td>\n",
       "      <td>111.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8024.150000</td>\n",
       "      <td>58228.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4.314400</td>\n",
       "      <td>7599.90000</td>\n",
       "      <td>508.740000</td>\n",
       "      <td>554.720000</td>\n",
       "      <td>158.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9163.200000</td>\n",
       "      <td>59647.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>6.507200</td>\n",
       "      <td>8408.10000</td>\n",
       "      <td>641.580000</td>\n",
       "      <td>643.070000</td>\n",
       "      <td>198.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9866.600000</td>\n",
       "      <td>60286.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>7.180550</td>\n",
       "      <td>11132.60000</td>\n",
       "      <td>922.995000</td>\n",
       "      <td>1064.220000</td>\n",
       "      <td>201.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10350.000000</td>\n",
       "      <td>62858.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>35038.00000</td>\n",
       "      <td>1346.000000</td>\n",
       "      <td>12727.000000</td>\n",
       "      <td>218.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y   z  frame  uncertainty    intensity  \\\n",
       "count     11.000000     11.000000  11     11    11.000000     11.00000   \n",
       "mean    8994.581818  59467.181818   0     50     5.993009  10992.20000   \n",
       "std     1170.696295   1687.184034   0      0     3.013617   8734.24533   \n",
       "min     6770.000000  56713.000000   0     50     1.078700   3107.80000   \n",
       "25%     8024.150000  58228.500000   0     50     4.314400   7599.90000   \n",
       "50%     9163.200000  59647.000000   0     50     6.507200   8408.10000   \n",
       "75%     9866.600000  60286.000000   0     50     7.180550  11132.60000   \n",
       "max    10350.000000  62858.000000   0     50    10.883000  35038.00000   \n",
       "\n",
       "            offset  loglikelihood       sigma  \n",
       "count    11.000000      11.000000   11.000000  \n",
       "mean    720.831818    1847.315455  179.280000  \n",
       "std     367.812667    3631.486533   39.753501  \n",
       "min     270.240000     243.080000  111.560000  \n",
       "25%     508.740000     554.720000  158.095000  \n",
       "50%     641.580000     643.070000  198.220000  \n",
       "75%     922.995000    1064.220000  201.995000  \n",
       "max    1346.000000   12727.000000  218.790000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the SimpleParser\n",
    "sp = parsers.SimpleParser()\n",
    "\n",
    "# Specify a file to parse\n",
    "file = pathToFiles / Path('HeLaL_Control_1.csv')\n",
    "\n",
    "# Parse this file\n",
    "sp.parseFilename(file, datasetType = 'locResults')\n",
    "\n",
    "# Summarize the localization data\n",
    "sp.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeLaL_Control\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>intensity</th>\n",
       "      <th>offset</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8994.581818</td>\n",
       "      <td>59467.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>5.993009</td>\n",
       "      <td>10992.20000</td>\n",
       "      <td>720.831818</td>\n",
       "      <td>1847.315455</td>\n",
       "      <td>179.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1170.696295</td>\n",
       "      <td>1687.184034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.013617</td>\n",
       "      <td>8734.24533</td>\n",
       "      <td>367.812667</td>\n",
       "      <td>3631.486533</td>\n",
       "      <td>39.753501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6770.000000</td>\n",
       "      <td>56713.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.078700</td>\n",
       "      <td>3107.80000</td>\n",
       "      <td>270.240000</td>\n",
       "      <td>243.080000</td>\n",
       "      <td>111.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8024.150000</td>\n",
       "      <td>58228.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4.314400</td>\n",
       "      <td>7599.90000</td>\n",
       "      <td>508.740000</td>\n",
       "      <td>554.720000</td>\n",
       "      <td>158.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9163.200000</td>\n",
       "      <td>59647.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>6.507200</td>\n",
       "      <td>8408.10000</td>\n",
       "      <td>641.580000</td>\n",
       "      <td>643.070000</td>\n",
       "      <td>198.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9866.600000</td>\n",
       "      <td>60286.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>7.180550</td>\n",
       "      <td>11132.60000</td>\n",
       "      <td>922.995000</td>\n",
       "      <td>1064.220000</td>\n",
       "      <td>201.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10350.000000</td>\n",
       "      <td>62858.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>35038.00000</td>\n",
       "      <td>1346.000000</td>\n",
       "      <td>12727.000000</td>\n",
       "      <td>218.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y   z  frame  uncertainty    intensity  \\\n",
       "count     11.000000     11.000000  11     11    11.000000     11.00000   \n",
       "mean    8994.581818  59467.181818   0     50     5.993009  10992.20000   \n",
       "std     1170.696295   1687.184034   0      0     3.013617   8734.24533   \n",
       "min     6770.000000  56713.000000   0     50     1.078700   3107.80000   \n",
       "25%     8024.150000  58228.500000   0     50     4.314400   7599.90000   \n",
       "50%     9163.200000  59647.000000   0     50     6.507200   8408.10000   \n",
       "75%     9866.600000  60286.000000   0     50     7.180550  11132.60000   \n",
       "max    10350.000000  62858.000000   0     50    10.883000  35038.00000   \n",
       "\n",
       "            offset  loglikelihood       sigma  \n",
       "count    11.000000      11.000000   11.000000  \n",
       "mean    720.831818    1847.315455  179.280000  \n",
       "std     367.812667    3631.486533   39.753501  \n",
       "min     270.240000     243.080000  111.560000  \n",
       "25%     508.740000     554.720000  158.095000  \n",
       "50%     641.580000     643.070000  198.220000  \n",
       "75%     922.995000    1064.220000  201.995000  \n",
       "max    1346.000000   12727.000000  218.790000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a Dataset that can be inserted into a B-Store database\n",
    "ds = sp.getDatabaseAtom()\n",
    "print(ds.prefix)\n",
    "print(ds.acqID)\n",
    "ds.data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- A `Parser` reads raw data files and converts them into a format for insertion into a B-Store database.\n",
    "- A `SimpleParser` is built-in into B-Store already.\n",
    "- The `SimpleParser` knows how to read files of the format **prefix**\\_**acqID**.filetype\n",
    "- When writing a `Parser`, you need to specify at least three functions from the `Parser` interface: `parseFilename()`, `getDatabaseAtom()`, and the `data` property.\n",
    "- `parseFilename()` knows how to extract B-Store identifiers from files.\n",
    "- `getDatabaseAtom()` returns a `Dataset` object, which implements the `DatabaseAtom` interface.\n",
    "- `data` uses the `@property` decorator and tells the `SimpleParser` how to read the data in the files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
