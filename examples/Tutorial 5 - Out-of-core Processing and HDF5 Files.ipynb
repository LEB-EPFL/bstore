{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the normal BatchProcessor class, there is class called H5BatchProcessor for working with the HDF5 file format. This format allows data to be written in a table format to disk in a fast an efficient manner. More importantly, database-like queries may be made to the file so that data is selectively read back into memory. These features, along with HDF's popularity in data-intensive science makes the HDF format a useful one for storing very large localizations files.\n",
    "\n",
    "The H5BatchProcessor can take either a .csv file (like the .dat files that the Fang software outputs) or .h5 file as input. It produces an .h5 file as output. Inside the .h5 file, multiple tables may be stored, so that filtered and merged localizations may be kept in the same file as the raw localizations.\n",
    "\n",
    "The Python Pandas library provides extremely fast functions for writing and reading these files; the DataSTORM library is essentially a wrapper around Pandas to make the Pandas code easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import DataSTORM.processors as proc\n",
    "import DataSTORM.batch      as bat\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the H5BatchProcessor\n",
    "The H5BatchProcessor works in a similar manner as BatchProcessor but with a few extra features. It allows the user to set a chunk size so that only a small amount of data is read and processed at time. If the datasets are small, then the whole dataset may be loaded and processed in memory by setting `chunksize = None`.\n",
    "\n",
    "Pipelines are constructed in a manner similar to Tutorial 2. The one difference is that only Filter and CleanUp are supported when the `chunksize` is something other than None. The reason for this is that out-of-core processing requires special algorithms for processing data on disk.\n",
    "\n",
    "H5BatchProcessor provides one additional method called `goMerge()`. This method is used to merge datasets that are too large to fit inside memory. The downside to this method is that it takes an extremely long time to merge localization data from the disk; for this reason, it should only be used in extreme cases. For all other cases where the data can fit inside memory, Merging may be performed by the H5BatchProcessor or regular BatchProcessor by placing it in the pipeline like normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up data from a .csv file and save to .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanup  = proc.CleanUp()\n",
    "pipeline = [cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputDir = Path('../test-data/Centrioles/')\n",
    "bp = bat.H5BatchProcessor(inputDir,\n",
    "                          pipeline,\n",
    "                          suffix        = '_locResults_small.dat', # Look for files ending with these\n",
    "                          chunksize     = 2e6,         # Number of localizations in a chunk; set to None to load all localizations into memory\n",
    "                          inputFileType = 'csv',       # Can be either 'csv' or 'h5\n",
    "                          useSameFolder = True,        # Save results to the same folder as the input datafiles\n",
    "                          outputKey     = 'processed') # Optional: this identifies the table inside the h5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline on the data\n",
    "Since the pipeline is just a CleanUp processor, the data will be cleaned up and stored in an h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp.go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform out-of-core merging\n",
    "This step is only necessary if the data is too large to fit into memory on your machine. It continually reads from and writes to the h5 file on the disk. Because it takes a long time, it is recommended to do merging in the pipeline of the `BatchProcessor.go()` method if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 889: 28 trajectories present\n",
      "CPU times: user 3min 33s, sys: 7.62 s, total: 3min 41s\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%time bp.goMerge(mergeRadius = 40,\n",
    "                 tOff        = 1,\n",
    "                 writeChunks = 10000) # This determines how many trajectories to compute statistics for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the processed and merged data\n",
    "The original data will be retained in a table, most likely named `processed` unless the user changes this. The merged and processed data is stored in a table named `merged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file     = Path('../test-data/Centrioles/FOV_1_noPB_1500mW_10ms_1/FOV_1_noPB_1500mW_10ms_1_MMStack_locResults_small_processed.h5')\n",
    "\n",
    "procedData = pd.read_hdf(str(file), key = 'processed') # Read the processed--but unmerged--localizations\n",
    "mergedData = pd.read_hdf(str(file), key = 'merged')    # Read the merged data from the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "      <th>precision</th>\n",
       "      <th>photons</th>\n",
       "      <th>bg</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747</td>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747.000000</td>\n",
       "      <td>53747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27697.949764</td>\n",
       "      <td>31415.551339</td>\n",
       "      <td>0</td>\n",
       "      <td>497.493386</td>\n",
       "      <td>198.025954</td>\n",
       "      <td>4453.983217</td>\n",
       "      <td>235.304148</td>\n",
       "      <td>242.865845</td>\n",
       "      <td>138.299916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17617.999474</td>\n",
       "      <td>18810.862273</td>\n",
       "      <td>0</td>\n",
       "      <td>236.054761</td>\n",
       "      <td>5349.492832</td>\n",
       "      <td>2776.706977</td>\n",
       "      <td>61.389436</td>\n",
       "      <td>403.084408</td>\n",
       "      <td>17.146917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>130.560000</td>\n",
       "      <td>6.247900</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.144000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>147.370000</td>\n",
       "      <td>-15.715000</td>\n",
       "      <td>87.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10539.000000</td>\n",
       "      <td>13739.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>3.780400</td>\n",
       "      <td>2465.400000</td>\n",
       "      <td>211.590000</td>\n",
       "      <td>86.140500</td>\n",
       "      <td>129.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26753.000000</td>\n",
       "      <td>29162.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>488.000000</td>\n",
       "      <td>5.148000</td>\n",
       "      <td>3501.100000</td>\n",
       "      <td>226.980000</td>\n",
       "      <td>115.950000</td>\n",
       "      <td>135.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40249.500000</td>\n",
       "      <td>48693.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>693.000000</td>\n",
       "      <td>6.657200</td>\n",
       "      <td>5539.550000</td>\n",
       "      <td>240.870000</td>\n",
       "      <td>190.650000</td>\n",
       "      <td>143.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64881.000000</td>\n",
       "      <td>64961.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>172430.000000</td>\n",
       "      <td>39686.000000</td>\n",
       "      <td>953.390000</td>\n",
       "      <td>9072.800000</td>\n",
       "      <td>378.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y      z         frame      precision  \\\n",
       "count  53747.000000  53747.000000  53747  53747.000000   53747.000000   \n",
       "mean   27697.949764  31415.551339      0    497.493386     198.025954   \n",
       "std    17617.999474  18810.862273      0    236.054761    5349.492832   \n",
       "min      130.560000      6.247900      0    100.000000       1.144000   \n",
       "25%    10539.000000  13739.500000      0    296.000000       3.780400   \n",
       "50%    26753.000000  29162.000000      0    488.000000       5.148000   \n",
       "75%    40249.500000  48693.000000      0    693.000000       6.657200   \n",
       "max    64881.000000  64961.000000      0    940.000000  172430.000000   \n",
       "\n",
       "            photons            bg  loglikelihood         sigma  \n",
       "count  53747.000000  53747.000000   53747.000000  53747.000000  \n",
       "mean    4453.983217    235.304148     242.865845    138.299916  \n",
       "std     2776.706977     61.389436     403.084408     17.146917  \n",
       "min        1.000000    147.370000     -15.715000     87.197000  \n",
       "25%     2465.400000    211.590000      86.140500    129.010000  \n",
       "50%     3501.100000    226.980000     115.950000    135.730000  \n",
       "75%     5539.550000    240.870000     190.650000    143.880000  \n",
       "max    39686.000000    953.390000    9072.800000    378.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procedData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>bg</th>\n",
       "      <th>photons</th>\n",
       "      <th>frame</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13157.000000</td>\n",
       "      <td>13157.000000</td>\n",
       "      <td>13157</td>\n",
       "      <td>13157.000000</td>\n",
       "      <td>13157.000000</td>\n",
       "      <td>13157.000000</td>\n",
       "      <td>13157.000000</td>\n",
       "      <td>13157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27351.949890</td>\n",
       "      <td>28669.883848</td>\n",
       "      <td>0</td>\n",
       "      <td>346.202615</td>\n",
       "      <td>961.214737</td>\n",
       "      <td>18194.293583</td>\n",
       "      <td>490.565250</td>\n",
       "      <td>4.084974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16960.974950</td>\n",
       "      <td>18237.781763</td>\n",
       "      <td>0</td>\n",
       "      <td>510.652373</td>\n",
       "      <td>2848.105135</td>\n",
       "      <td>61815.535802</td>\n",
       "      <td>237.541356</td>\n",
       "      <td>13.140657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>133.361549</td>\n",
       "      <td>8.103771</td>\n",
       "      <td>0</td>\n",
       "      <td>18.378000</td>\n",
       "      <td>152.120000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10565.852533</td>\n",
       "      <td>9995.613906</td>\n",
       "      <td>0</td>\n",
       "      <td>91.717000</td>\n",
       "      <td>258.930000</td>\n",
       "      <td>5073.800000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26224.000000</td>\n",
       "      <td>26895.223398</td>\n",
       "      <td>0</td>\n",
       "      <td>126.460000</td>\n",
       "      <td>501.230000</td>\n",
       "      <td>9573.800000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39531.000000</td>\n",
       "      <td>40788.864662</td>\n",
       "      <td>0</td>\n",
       "      <td>376.123333</td>\n",
       "      <td>1111.320000</td>\n",
       "      <td>20367.200000</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64881.000000</td>\n",
       "      <td>64961.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>8829.866667</td>\n",
       "      <td>180336.790000</td>\n",
       "      <td>5152339.200000</td>\n",
       "      <td>939.000000</td>\n",
       "      <td>840.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y      z  loglikelihood             bg  \\\n",
       "count  13157.000000  13157.000000  13157   13157.000000   13157.000000   \n",
       "mean   27351.949890  28669.883848      0     346.202615     961.214737   \n",
       "std    16960.974950  18237.781763      0     510.652373    2848.105135   \n",
       "min      133.361549      8.103771      0      18.378000     152.120000   \n",
       "25%    10565.852533   9995.613906      0      91.717000     258.930000   \n",
       "50%    26224.000000  26895.223398      0     126.460000     501.230000   \n",
       "75%    39531.000000  40788.864662      0     376.123333    1111.320000   \n",
       "max    64881.000000  64961.000000      0    8829.866667  180336.790000   \n",
       "\n",
       "              photons         frame        length  \n",
       "count    13157.000000  13157.000000  13157.000000  \n",
       "mean     18194.293583    490.565250      4.084974  \n",
       "std      61815.535802    237.541356     13.140657  \n",
       "min          1.000000    100.000000      1.000000  \n",
       "25%       5073.800000    283.000000      1.000000  \n",
       "50%       9573.800000    482.000000      2.000000  \n",
       "75%      20367.200000    682.000000      5.000000  \n",
       "max    5152339.200000    939.000000    840.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You will notice that the column names have changed. The reason for this is that HDF files can only be queried from disk if they do not contain spaces in their names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the merged data to a .csv\n",
    "It may be convenient to save the merged data back to a csv so one may, for example, render the data in ThunderSTORM. To do this, we can simply convert the header back to ThunderSTORM format and save the DataFrame as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert = proc.ConvertHeader(proc.FormatLEB(), proc.FormatThunderSTORM())\n",
    "newDF   = convert(mergedData)\n",
    "\n",
    "# Save to the same directory as this notebook\n",
    "newDF.to_csv('mergedData.csv', index = False) # `index = false` means that the linked particle ID will not be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
