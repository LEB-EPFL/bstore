{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing custom parsers\n",
    "B-Store was designed to work with your data by not enforcing strict rules about file formats. This means, for example, that you are not required to follow a certain column naming convention or to use .csv files when generating your raw data.\n",
    "\n",
    "While this gives you a lot of flexibility when acquiring your data in the lab, it does come at a cost: you might need to write your own parser or dataset type.\n",
    "\n",
    "B-Store comes with two built-in parsers known as a `SimpleParser` and `PositionParser` to provide out-of-the-box functionality for simple datasets. In this tutorial, we'll write the SimpleParser from scratch to demonstrate how you may write your own parsers for B-Store.\n",
    "\n",
    "## The logic of B-Store\n",
    "B-Store was designed to take localization data, widefield images, metadata, and more and convert them into a format that is easily stored for both human and machine interpretation. This logic is illustrated below:\n",
    "\n",
    "<img src=\"../images/dataset_logic.png\" width = 50%/>\n",
    "\n",
    "The role of the `Parser` is take these raw datasets and assign to them a descriptive name (known as a `prefix`) that identifies datasets that should be grouped together, such as grouping data from controls and treatments into separate groups. Within these groups, which are known as acquisition groups, each dataset is identified by a number known as the `acqID` and the type of data it contains, the `datasetType`. Finally, there are a number of other fields that may identify the dataset if more precise delimitation between datasets is required.\n",
    "\n",
    "When provided with a file, a `Parser` is required to specify the following fields:\n",
    "\n",
    "- `acqID` - a unique integer for a given prefix\n",
    "- `prefix` - a string that gives a descriptive name to the dataset\n",
    "- `datasetType` - this is actually specified by the user, but used by the `Parser` to know how to read the data from a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Parser` interface\n",
    "The reason that B-Store needs this ID information is that organization in the datastore can be automated only if the data matches the datastore interface. In B-Store, this interface is known as a `DatasetID`.\n",
    "\n",
    "To ease its creation, a parser must also implement an interface known as a `Parser`. The `Parser` interface is simply a list of functions that a Python class must implement to be called a `Parser`. Let's start by looking at the code for this interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import B-Store's parsers module\n",
    "from bstore import parsers\n",
    "\n",
    "# Used to retrieve the code\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Parser(metaclass = ABCMeta):\n",
      "    \"\"\"Translates SMLM files to machine-readable data structures.\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    dataset  : Dataset\n",
      "        A Dataset object for insertion into a B-Store Datastore.\n",
      "    requiresConfig : bool\n",
      "        Does parser require configuration before use? This is primarily\n",
      "        used by the GUI to determine whether the parser has attributes that\n",
      "        are set by its __init__() method or must be set before parsing files.\n",
      "       \n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        # Holds a parsed dataset.\n",
      "        self._dataset = None\n",
      "    \n",
      "    @property\n",
      "    def dataset(self):\n",
      "        if self._dataset:\n",
      "            return self._dataset\n",
      "        else:\n",
      "            raise ParserNotInitializedError('Error: There is currently no'\n",
      "                                            'parsed dataset to return.')\n",
      "        \n",
      "    @dataset.setter\n",
      "    def dataset(self, ds):\n",
      "        self._dataset = ds\n",
      "    \n",
      "    @abstractproperty\n",
      "    def requiresConfig(self):\n",
      "        pass    \n",
      "    \n",
      "    @abstractmethod\n",
      "    def parseFilename(self):\n",
      "        \"\"\"Parses a file for conversion to a Dataset.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(parsers.Parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the code above, we can see that a `Parser` has one function:\n",
    "\n",
    "- `__init__()` : the constructor that assigns the class fields\n",
    "\n",
    "There is also an attribute known as a `dataset` that contains the dataset object after a filename has been parsed.\n",
    "\n",
    "Python uses the decorators `abstractproperty` or `abstractmethod` to identify attributes and functions that a real Parser instance must provide (in this metaclass their body's contents only contain the word `pass`).\n",
    "\n",
    "- `requiresConfig` - Used by the GUI to indicate whether a popup window will appear for configuring the parser\n",
    "- `parseFilename` - generates the DatabaseAtom ID fields from a file or filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the `SimpleParser`\n",
    "\n",
    "## File naming conventions\n",
    "For the sake of this tutorial, let's suppose that our acquisition software produces files that follow this naming convention:\n",
    "\n",
    "- **prefix_acqID.csv** : `locResults` come in .csv files that with a common name, followed by an underscore, and then an integer identifier. For example, HeLa_2.csv\n",
    "- **prefix_acqID.txt** : `locMetadata` is found in .txt files with prefixes and acquisition ID's that match their corresponding localization data\n",
    "- **prefix_acqID.tif** : `widefieldImage`'s are found in tif files that also match the corresponding the localization data.\n",
    "\n",
    "## SimpleParser inputs and outputs\n",
    "Our `SimpleParser` will be relatively, well, simple to convert these files into a format that B-Store can organize. This will hopefully give you the main idea about how you may write your own and provide a base class for doing so.\n",
    "\n",
    "The parser's constructor will take no arguments. It's main function, `parseFilename()` will take a string as input that represents a file's name and another string representing the `datasetType` of the file. This function will set the ID fields of the `Parser` and also tell the Parser how to read the data.\n",
    "\n",
    "Let's write an outline of this class following this design that doesn't actually do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "class SimpleParser(Parser):\n",
    "    \"\"\"A simple parser for extracting acquisition information.\n",
    "    \n",
    "    The SimpleParser converts files of the format prefix_acqID.* into\n",
    "    DatabaseAtoms for insertion into a database. * may represent .csv files\n",
    "    (for locResults), .json (for locMetadata), and .tif (for widefieldImages).\n",
    "    \n",
    "    \"\"\"\n",
    "    @property\n",
    "    def requiresConfig(self):\n",
    "        return False\n",
    "    \n",
    "    def parseFilename(self):\n",
    "        pass\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the skeleton above we have all the elements that are required by the interface. The problem is, there's no actual functionality at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `parseFilename()`\n",
    "Most of the work done by the Parser is the `parseFilename()` function. This function reads a filename and then fills in the appropriate fields of `Parser` parent class, like `acqID`, `prefix`, etc. The function should also take an argument that we'll call `datasetType` that tells it what kind of dataset it's looking at. The function then handles each type of dataset differently.\n",
    "\n",
    "Let's add this argument and another named `filename`, then begin to flesh out the function.\n",
    "\n",
    "```python\n",
    "def parseFilename(self, filename, datasetType = 'Localizations', **kwargs):\n",
    "        \"\"\"Converts a filename into a Dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename      : str or Path\n",
    "            A string or pathlib Path object containing the dataset's filename.\n",
    "        datasetType   : str\n",
    "            The type of the dataset being parsed. This tells the Parser\n",
    "            how to interpret the data.\n",
    "            \n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll reset the parser by setting the `dataset` (provided by the `Parser` interface) to None.\n",
    "\n",
    "```python\n",
    "# Resets the parser\n",
    "        self.dataset = None  \n",
    "```\n",
    "\n",
    "Next, we need to check that the dataset type provided as argument is currently registered. The reason that B-Store requires type registration is that it helps prevent reading and writing unwanted file types when traversing directories of raw data. Some unwanted files could accidentally sneak into the Datastore if their naming pattern matched that of a dataset type. Note that `config` refers to `bstore.config` and must be imported in the file for this code.\n",
    "\n",
    "```python\n",
    " # Check for a valid datasetType\n",
    "        if datasetType not in config.__Registered_DatasetTypes__:\n",
    "            raise DatasetTypeError(('{} is not a registered '\n",
    "                                    'type.').format(datasetType))\n",
    "```\n",
    "\n",
    "The full path to the filename is saved for later and Path objects are converted to strings:\n",
    "\n",
    "```python\n",
    "# Save the full path to the file for later.\n",
    "# If filename is already a Path object, this does nothing.\n",
    "self._fullPath = pathlib.Path(filename)        \n",
    "\n",
    "# Convert Path objects to strings if Path is supplied\n",
    "if isinstance(filename, pathlib.PurePath):\n",
    "    filename = str(filename.name)\n",
    "```\n",
    "\n",
    "Now let's look again briefly at the naming convention of our data. All of our files follow the rule **prefix_acqID.xxx**. This means that the file type--.csv, .txt, or .tif--already tells us the dataset type. The first part of the filename will always tell us the `prefix`, which can be anything, and the last part will always be an underscore followed by an integer `acqID`.\n",
    "\n",
    "We can easily extract this information with Python's built-in string manipulation tools and the *os.path* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove the file type: path/to/HeLa_Control_7\n",
      "Remove the file type and parent folders: HeLa_Control_7\n",
      "HeLa_Control_7\n"
     ]
    }
   ],
   "source": [
    "from os.path import splitext\n",
    "\n",
    "# Example\n",
    "filename = 'path/to/HeLa_Control_7.csv'\n",
    "\n",
    "# Remove the '.csv'\n",
    "print('Remove the file type: ' + splitext(filename)[0])\n",
    "\n",
    "# Remove any parent folders\n",
    "print('Remove the file type and parent folders: ' + splitext(filename)[0].split('/')[-1])\n",
    "\n",
    "# This works if there are no parents folders, too\n",
    "print(splitext('HeLa_Control_7.csv')[0].split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prefix` and `acqID` values are easy to get. We simply split the string at the last underscore and take the part before it as the `prefix` and the part after as the `acqID`. Python's `rsplit()` function does this for us. Finally, we convert the `acqID` from a string to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix is: HeLa_Control\n",
      "acqID is: 7\n"
     ]
    }
   ],
   "source": [
    "# Isolate the root filename\n",
    "rootName = splitext(filename)[0].split('/')[-1]\n",
    "\n",
    "# Split the string at the last underscore\n",
    "prefix, acqID = rootName.rsplit('_', 1)\n",
    "acqID = int(acqID) # Convert the string to an integer\n",
    "\n",
    "print('prefix is: {:s}'.format(prefix))\n",
    "print('acqID is: {:d}'.format(acqID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datasetType` was already an input to the `parseFilename()` function, so we don't need to do anything to get it from the filename.\n",
    "\n",
    "Now we have all of the ID's that parser is designed to interpret: `prefix`, `acqID`, and `datasetType`. The other ID's, which are `channelID`, `dateID`, `posID`, and `sliceID`, are optional and can be implemented in your own parser. The SimpleParser will not assign values to them.\n",
    "\n",
    "We finish the function by building the return dataset and reading the data from the file.\n",
    "\n",
    "```python\n",
    "# Build the return dataset\n",
    "idDict = {'prefix' : prefix, 'acqID' : acqID}\n",
    "\n",
    "mod   = importlib.import_module(\n",
    "    'bstore.datasetTypes.{0:s}'.format(datasetType))\n",
    "dType             = getattr(mod, datasetType)\n",
    "self.dataset      = dType(datasetIDs = idDict)\n",
    "self.dataset.data = self.dataset.readFromFile(self._fullPath)\n",
    "```\n",
    "\n",
    "DatasetTypes are each stored in a file of the same name. The line containing `importlib` imports this file much like you would using the line `import bstore.datasetType.TYPE_NAME`. `dType` is an actual object of the class representing the datasetType. The last two lines create the instance of the datasetType and read the data from the file.\n",
    "\n",
    "The full `parseFilename` function for `SimpleParser` looks like what follows below. The whole code block is wrapped inside a try...except statement in case an error is raised during parsing. If an error is raised, the `self.dataset` field is set to None.\n",
    "\n",
    "```python\n",
    "    def parseFilename(self, filename, datasetType = 'locResults'):\n",
    "        \"\"\"Converts a filename into a DatabaseAtom.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename      : str or Path\n",
    "            A string or pathlib Path object containing the dataset's filename.\n",
    "        datasetType   : str\n",
    "            The type of the dataset being parsed. This tells the Parser\n",
    "            how to interpret the data.\n",
    "            \n",
    "        \"\"\"\n",
    "        # Resets the parser\n",
    "        self.dataset = None        \n",
    "        \n",
    "        # Check for a valid datasetType\n",
    "        if datasetType not in config.__Registered_DatasetTypes__:\n",
    "            raise DatasetTypeError(('{} is not a registered '\n",
    "                                    'type.').format(datasetType))     \n",
    "        \n",
    "        try:\n",
    "            # Save the full path to the file for later.\n",
    "            # If filename is already a Path object, this does nothing.\n",
    "            self._fullPath = pathlib.Path(filename)        \n",
    "            \n",
    "            # Convert Path objects to strings if Path is supplied\n",
    "            if isinstance(filename, pathlib.PurePath):\n",
    "                filename = str(filename.name)\n",
    "    \n",
    "            # Remove file type ending and any parent folders\n",
    "            # Example: 'path/to/HeLa_Control_7.csv' becomes 'HeLa_Control_7'\n",
    "            rootName = splitext(filename)[0].split('/')[-1]\n",
    "            \n",
    "            # Extract the prefix and acqID\n",
    "            prefix, acqID = rootName.rsplit('_', 1)\n",
    "            acqID = int(acqID)\n",
    "            \n",
    "            # Build the return dataset\n",
    "            idDict = {'prefix' : prefix, 'acqID' : acqID}\n",
    "        \n",
    "            mod   = importlib.import_module(\n",
    "                'bstore.datasetTypes.{0:s}'.format(datasetType))\n",
    "            dType             = getattr(mod, datasetType)\n",
    "            self.dataset      = dType(datasetIDs = idDict)\n",
    "            self.dataset.data = self.dataset.readFromFile(self._fullPath)\n",
    "        except:\n",
    "            self.dataset = None\n",
    "            raise ParseFilenameFailure(('Error: File could not be parsed.',\n",
    "                                        sys.exc_info()[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `SimpleParser` class definition\n",
    "Here is the final, full definition for our `SimpleParser` class.\n",
    "\n",
    "```python\n",
    "class SimpleParser(Parser):\n",
    "    \"\"\"A simple parser for extracting acquisition information.\n",
    "    \n",
    "    The SimpleParser converts files of the format prefix_acqID.* into\n",
    "    Datasets for insertion into a datastore. * represents filename\n",
    "    extensions like .csv, .json, and .tif.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    requiresConfig : bool\n",
    "        Does parser require configuration before use?\n",
    "    \n",
    "    \"\"\"\n",
    "    @property\n",
    "    def requiresConfig(self):\n",
    "        return False\n",
    "        \n",
    "    def parseFilename(self, filename, datasetType = 'Localizations', **kwargs):\n",
    "        \"\"\"Converts a filename into a Dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename      : str or Path\n",
    "            A string or pathlib Path object containing the dataset's filename.\n",
    "        datasetType   : str\n",
    "            The type of the dataset being parsed. This tells the Parser\n",
    "            how to interpret the data.\n",
    "            \n",
    "        \"\"\"\n",
    "        # Resets the parser\n",
    "        self.dataset = None        \n",
    "        \n",
    "        # Check for a valid datasetType\n",
    "        if datasetType not in config.__Registered_DatasetTypes__:\n",
    "            raise DatasetTypeError(('{} is not a registered '\n",
    "                                    'type.').format(datasetType))     \n",
    "        \n",
    "        try:\n",
    "            # Save the full path to the file for later.\n",
    "            # If filename is already a Path object, this does nothing.\n",
    "            self._fullPath = pathlib.Path(filename)        \n",
    "            \n",
    "            # Convert Path objects to strings if Path is supplied\n",
    "            if isinstance(filename, pathlib.PurePath):\n",
    "                filename = str(filename.name)\n",
    "    \n",
    "            # Remove file type ending and any parent folders\n",
    "            # Example: 'path/to/HeLa_Control_7.csv' becomes 'HeLa_Control_7'\n",
    "            rootName = splitext(filename)[0].split('/')[-1]\n",
    "            \n",
    "            # Extract the prefix and acqID\n",
    "            prefix, acqID = rootName.rsplit('_', 1)\n",
    "            acqID = int(acqID)\n",
    "            \n",
    "            # Build the return dataset\n",
    "            idDict = {'prefix' : prefix, 'acqID' : acqID}\n",
    "        \n",
    "            mod   = importlib.import_module(\n",
    "                'bstore.datasetTypes.{0:s}'.format(datasetType))\n",
    "            dType             = getattr(mod, datasetType)\n",
    "            self.dataset      = dType(datasetIDs = idDict)\n",
    "            self.dataset.data = self.dataset.readFromFile(self._fullPath)\n",
    "        except:\n",
    "            self.dataset = None\n",
    "            raise ParseFilenameFailure(('Error: File could not be parsed.',\n",
    "                                        sys.exc_info()[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "For this example, you can use the test data in the [bstore_test_files](https://github.com/kmdouglass/bstore_test_files). Download the files from Git using the link and change the path below to point to *parsers_test_files/SimpleParsers* on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import bstore.config\n",
    "\n",
    "# Register the dataset types\n",
    "bstore.config.__Registered_DatasetTypes__ = ['Localizations', 'LocMetadata', 'WidefieldImage']\n",
    "\n",
    "# Specify the test dataset\n",
    "pathToFiles = Path('../../bstore_test_files/parsers_test_files/SimpleParser/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localizations: {'prefix': 'HeLaL_Control', 'acqID': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create the SimpleParser\n",
    "sp = parsers.SimpleParser()\n",
    "\n",
    "# Specify a file to parse\n",
    "file = pathToFiles / Path('HeLaL_Control_1.csv')\n",
    "\n",
    "# Parse this file\n",
    "sp.parseFilename(file, datasetType = 'Localizations')\n",
    "\n",
    "# Summarize the localization data\n",
    "print(sp.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>intensity</th>\n",
       "      <th>offset</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8994.581818</td>\n",
       "      <td>59467.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.993009</td>\n",
       "      <td>10992.20000</td>\n",
       "      <td>720.831818</td>\n",
       "      <td>1847.315455</td>\n",
       "      <td>179.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1170.696295</td>\n",
       "      <td>1687.184034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.013617</td>\n",
       "      <td>8734.24533</td>\n",
       "      <td>367.812667</td>\n",
       "      <td>3631.486533</td>\n",
       "      <td>39.753501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6770.000000</td>\n",
       "      <td>56713.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.078700</td>\n",
       "      <td>3107.80000</td>\n",
       "      <td>270.240000</td>\n",
       "      <td>243.080000</td>\n",
       "      <td>111.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8024.150000</td>\n",
       "      <td>58228.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.314400</td>\n",
       "      <td>7599.90000</td>\n",
       "      <td>508.740000</td>\n",
       "      <td>554.720000</td>\n",
       "      <td>158.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9163.200000</td>\n",
       "      <td>59647.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.507200</td>\n",
       "      <td>8408.10000</td>\n",
       "      <td>641.580000</td>\n",
       "      <td>643.070000</td>\n",
       "      <td>198.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9866.600000</td>\n",
       "      <td>60286.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.180550</td>\n",
       "      <td>11132.60000</td>\n",
       "      <td>922.995000</td>\n",
       "      <td>1064.220000</td>\n",
       "      <td>201.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10350.000000</td>\n",
       "      <td>62858.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>35038.00000</td>\n",
       "      <td>1346.000000</td>\n",
       "      <td>12727.000000</td>\n",
       "      <td>218.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y     z  frame  uncertainty    intensity  \\\n",
       "count     11.000000     11.000000  11.0   11.0    11.000000     11.00000   \n",
       "mean    8994.581818  59467.181818   0.0   50.0     5.993009  10992.20000   \n",
       "std     1170.696295   1687.184034   0.0    0.0     3.013617   8734.24533   \n",
       "min     6770.000000  56713.000000   0.0   50.0     1.078700   3107.80000   \n",
       "25%     8024.150000  58228.500000   0.0   50.0     4.314400   7599.90000   \n",
       "50%     9163.200000  59647.000000   0.0   50.0     6.507200   8408.10000   \n",
       "75%     9866.600000  60286.000000   0.0   50.0     7.180550  11132.60000   \n",
       "max    10350.000000  62858.000000   0.0   50.0    10.883000  35038.00000   \n",
       "\n",
       "            offset  loglikelihood       sigma  \n",
       "count    11.000000      11.000000   11.000000  \n",
       "mean    720.831818    1847.315455  179.280000  \n",
       "std     367.812667    3631.486533   39.753501  \n",
       "min     270.240000     243.080000  111.560000  \n",
       "25%     508.740000     554.720000  158.095000  \n",
       "50%     641.580000     643.070000  198.220000  \n",
       "75%     922.995000    1064.220000  201.995000  \n",
       "max    1346.000000   12727.000000  218.790000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a Dataset that can be inserted into a B-Store database\n",
    "ds = sp.dataset.data\n",
    "ds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- A `Parser` reads raw data files and converts them into a format for insertion into a B-Store datastore.\n",
    "- A `SimpleParser` is built-in into B-Store already.\n",
    "- The `SimpleParser` knows how to read files of the format **prefix**\\_**acqID**.filetype\n",
    "- When writing a `Parser`, you need to specify at least two attributes from the `Parser` interface: `parseFilename()`, and `requiresConfig`.\n",
    "- `parseFilename()` knows how to extract B-Store identifiers from files.\n",
    "- The dataset that is parsed is stored in `parser.dataset`. The data from the file is inside `parser.dataset.data`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
