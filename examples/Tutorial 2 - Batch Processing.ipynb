{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to do batch processing on localization files that fit within memory. This should be the case for most applications. One exception is the very large datasets from large field of view microtubule data; these are sometimes too large to fit into memory and most be processed out-of-core.\n",
    "\n",
    "Because the drift correction step is interactive, it would occur before the steps taken here. There is another tutorial that describes how to do drift correction in batch. This tutorial assumes the data is already drift-corrected or does not need drift correction.\n",
    "\n",
    "### Import the software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import DataSTORM.processors as proc\n",
    "import DataSTORM.batch      as batch\n",
    "import pandas               as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of batch processing steps\n",
    "\n",
    "1. Define the list of operations that will be performed on the localizations\n",
    "2. Set the input and output folders\n",
    "3. Run the batch processor\n",
    "\n",
    "# Define the list of operations\n",
    "Let's say we have a folder which contains a few subfolders with localization files in them. Each localization file ends in `.dat` and we want to clean up the data in them, filter by the uncertainty, merge the localizations, and then filter by loglikelihood. We first need to define processors for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanup = proc.CleanUp()\n",
    "filter1 = proc.Filter('uncertainty [nm]', '<', 30)\n",
    "filter2 = proc.Filter('loglikelihood',    '<', 250)\n",
    "merger  = proc.Merge(tOff = 1, mergeRadius = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the processors, we build a pipeline by placing each processor in a Python list. The order of processors in the list will be the same order in which their applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [cleanup,\n",
    "            filter1,\n",
    "            merger,  # Data is merged before filter2 is applied!\n",
    "            filter2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the input and output directories\n",
    "The next thing that the batch processor needs to know is the parent directory and where it should save the data. We can set an output directory if we want, though this is optional. The default behavior is to save the data in a subfolder of the parent folder called `processed_data`. If we want the output files to be saved in the same folder(s) as the input files, we set the `useSameFolder` flag of BatchProcessor to True (see a few lines below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputData = Path('../test-data/Centrioles/') # All folders in this folder will be searched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the batch processor and start it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # We will use the default output directory\n",
    "bp = batch.BatchProcessor(inputData,\n",
    "                          pipeline,\n",
    "                          useSameFolder = False,\n",
    "                          suffix = 'locResults.dat') # Look for files ending in 'locResults.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to start the batch processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 101: 74 trajectories present\n"
     ]
    }
   ],
   "source": [
    "bp.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
