{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing custom parsers (WORK IN PROGRESS)\n",
    "B-Store was designed to work with your data by not enforcing strict rules about file formats. This means, for example, that you are not required to follow a certain column naming convention or to use .csv files when generating your raw data.\n",
    "\n",
    "While this gives you a lot of flexibility when acquiring your data in the lab, it does come at a cost: you must write your own parser to translate your files into a format that can be organized by B-Store.\n",
    "\n",
    "B-Store comes with a built-in parser known as a `SimpleParser` to provide out-of-the-box functionality for simple datasets. In this tutorial, we'll write the SimpleParser from scratch to demonstrate how you may write your own parsers for B-Store.\n",
    "\n",
    "## The logic of B-Store\n",
    "B-Store was designed to take localization data, widefield images, and metadata and convert them into a format that was easily stored for both human and machine interpretation. This logic is illustrated below:\n",
    "\n",
    "<img src=\"../design/dataset_logic.png\" width = 50%/>\n",
    "\n",
    "The role of the `Parser` is take these raw datasets and assign to them a name (known as a `prefix`) that identifies datasets that should be grouped together, such as grouping data from controls and treatments into separate groups. Within these groups, which are known as acquisition groups, each dataset is identified by a number known as the `acqID` and the type of data it contains, the `datasetType`. Finally, there are a number of other fields that may identify the dataset if more precise ID's are required.\n",
    "\n",
    "When provided with a file, a `Parser` is required to specify the following fields:\n",
    "\n",
    "- `acqID` - a unique integer for a given prefix\n",
    "- `prefix` - a string that gives a descriptive name to the dataset\n",
    "- `datasetType` - one of the strings listed in the `__Types_Of_Atoms__` variable in *config.py*; at the time of writing, these are 'locResults', 'locMetadata', or 'widefieldImage'\n",
    "\n",
    "Additionally, the `Parser` must provide a way to access the actual data contained in a file. Depending on the `datasetType`, the data from a file is represented internally as one of these data types after loading from memory:\n",
    "\n",
    "- `locResults` - [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "- `locMetadata` - [JSON](http://www.json.org/) string-value pairs\n",
    "- `widefieldImage` - 2D [Numpy](http://www.numpy.org/) array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Parser` interface\n",
    "The reason that B-Store needs this information is that organization in the database can be automated only if the data matches the database interface. In B-Store, this interface is known as a `DatabaseAtom`.\n",
    "\n",
    "To ease their creation, a parsers must also implement an interface known as a `Parser`. The `Parser` interface is simply a list of functions that a Python class must implement to be called a `Parser`. Let's start by looking at the code for this interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import B-Store's parsers module\n",
    "from bstore import parsers\n",
    "\n",
    "# Used to retrieve the code\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Parser(metaclass = ABCMeta):\n",
      "    \"\"\"Translates files to machine-readable data structures with acq. info.\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    acqID       : int\n",
      "        The number identifying the Multi-D acquisition for a given prefix name.\n",
      "    channelID   : str\n",
      "        The color channel associated with the dataset.\n",
      "    dateID      : str\n",
      "        The date of the acquistion in the format YYYY-mm-dd.\n",
      "    posID       : (int,) or (int, int)\n",
      "        The position identifier. It is a single element tuple if positions were\n",
      "        manually set; otherwise, it's a 2-tuple indicating the x and y\n",
      "        identifiers.\n",
      "    prefix      : str\n",
      "        The descriptive name given to the dataset by the user.\n",
      "    sliceID     : int\n",
      "        The number identifying the z-axis slice of the dataset.\n",
      "    datasetType : str\n",
      "        The type of data contained in the dataset. Can be one of 'locResults',\n",
      "        'locMetadata', or 'widefieldImage'.\n",
      "       \n",
      "    \"\"\"\n",
      "    def __init__(self, acqID, channelID, dateID,\n",
      "                 posID, prefix, sliceID, datasetType):\n",
      "        \"\"\"Initialize the parser's metadata information.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        acqID       : int\n",
      "            The number identifying the Multi-D acquisition for a given prefix\n",
      "            name.\n",
      "        channelID   : str\n",
      "            The color channel associated with the dataset.\n",
      "        dateID      : str\n",
      "            The date of the acquistion in the format YYYY-mm-dd.\n",
      "        posID       : int, or (int, int)\n",
      "            The position identifier. It is a single element tuple if positions\n",
      "            were manually set; otherwise, it's a 2-tuple indicating the x and y\n",
      "            identifiers.\n",
      "        prefix      : str\n",
      "            The descriptive name given to the dataset by the user.\n",
      "        sliceID     : int\n",
      "            The number identifying the z-axis slice of the dataset.\n",
      "        datasetType : str\n",
      "            The type of data contained in the dataset. Can be one of\n",
      "            'locResults', 'locMetadata', or 'widefieldImage'.\n",
      "        \n",
      "        \"\"\"\n",
      "        if datasetType not in database.typesOfAtoms:\n",
      "            raise DatasetError(datasetType)\n",
      "        \n",
      "        # These are the essential pieces of information to identify a dataset.\n",
      "        self.acqID       =       acqID\n",
      "        self.channelID   =   channelID\n",
      "        self.dateID      =      dateID\n",
      "        self.posID       =       posID\n",
      "        self.prefix      =      prefix\n",
      "        self.sliceID     =     sliceID\n",
      "        self.datasetType = datasetType\n",
      "        \n",
      "    def getBasicInfo(self):\n",
      "        \"\"\"Return a dictionary containing the basic dataset information.\n",
      "        \n",
      "        \"\"\"\n",
      "        basicInfo = {\n",
      "                     'acqID'         :       self.acqID,\n",
      "                     'channelID'     :   self.channelID,\n",
      "                     'dateID'        :      self.dateID,\n",
      "                     'posID'         :       self.posID,\n",
      "                     'prefix'        :      self.prefix,\n",
      "                     'sliceID'       :     self.sliceID,\n",
      "                     'datasetType'   : self.datasetType\n",
      "                     }\n",
      "                     \n",
      "        return basicInfo\n",
      "    \n",
      "    @property\n",
      "    def prefix(self):\n",
      "        return self._prefix\n",
      "        \n",
      "    @prefix.setter\n",
      "    def prefix(self, value):\n",
      "        if value:\n",
      "            # Replaces spaces with '_' in prefix.\n",
      "            # This avoids problems with spaces in PyTables\n",
      "            self._prefix = value.replace(' ', '_')\n",
      "    \n",
      "    @abstractproperty\n",
      "    def data(self):\n",
      "        \"\"\"Loads the data into memory and maps it to the correct format.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "    \n",
      "    @abstractmethod\n",
      "    def getDatabaseAtom(self):\n",
      "        \"\"\"Returns one atomic unit for insertion into the Database.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "    \n",
      "    @abstractmethod\n",
      "    def parseFilename(self):\n",
      "        \"\"\"Parses a file for conversion to a DatabaseAtom.\n",
      "        \n",
      "        \"\"\"\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(parsers.Parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the code above, we can see that a `Parser` has two functions:\n",
    "\n",
    "- `__init__()` : the constructor that assigns the class fields\n",
    "- `getBasicInfo()` : returns a dictionary with the Parser's information\n",
    "\n",
    "Furthermore, there are a few functions that are preceded by `abstractproperty` or `abstractmethod` that don't actually do anything (their body's contents only contain the word `pass`). These are the functions and properties that our custom `Parser` must define to work with our data. They are:\n",
    "\n",
    "- `data` - contains the actual data from a file\n",
    "- `getDatabaseAtom()` - returns a DatabaseAtom instance that can be put inside a B-Store database\n",
    "- `parseFilename` - generates the DatabaseAtom ID fields from a file or filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the `SimpleParser`\n",
    "\n",
    "## File naming conventions\n",
    "For the sake of this tutorial, let's suppose that our acquisition software produces the files that follow this naming convention:\n",
    "\n",
    "- **prefix_acqID.csv** : `locResults` come in .csv files that with a common name, followed by an underscore, and then an integer identifier. For example, HeLa_2.csv\n",
    "- **prefix_acqID.txt** : `locMetadata` is found in .txt files with prefixes and acquisition ID's that match their corresponding localization data\n",
    "- **prefix_acqID.tiff** : `widefieldImage`'s are found in tiff files that also match the corresponding the localization data.\n",
    "\n",
    "## SimpleParser inputs and outputs\n",
    "Our `SimpleParser` will be relatively simple to convert these files into a format that B-Store can organize. Hopefully this will give you the main idea about how you may write your own.\n",
    "\n",
    "The parser's constructor will take no arguments. It's main function, `parseFilename()` will take a string as input that represents a file's name and another string representing the `datasetType` of the file. This function will set the ID fields of the `Parser` and also tell the Parser how to read the data.\n",
    "\n",
    "Let's write an outline of this class following this design that doesn't actuall do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "class SimpleParser(Parser):\n",
    "    \"\"\"A simple parser for and extracting acquisition information.\n",
    "    \n",
    "    The SimpleParser converts files of the format prefix_acqID.* into\n",
    "    DatabaseAtoms for insertion into a database. * may represent .csv files\n",
    "    (for locResults), .json (for locMetadata), and .tiff (for widefieldImages).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getDatabaseAtom(self):\n",
    "        pass\n",
    "    \n",
    "    def parseFilename(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        pass \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the skeleton above we have all the functions and the `data` property that are required by the interface. The problem is, there's no actualy functionality at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The \\_\\_init\\_\\_ constructor\n",
    "Unlike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
