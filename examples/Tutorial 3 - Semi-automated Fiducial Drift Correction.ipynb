{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The need to interactively tune the fiducial marker detection and check its output means that drift correction should not be performed in batch. However, the drift correction process can be made simpler by using a notebook that assists in finding the files and performing the drift correction. This notebook serves as the template for this step.\n",
    "\n",
    "Note: Typically, this step would be performed before the actual batch processing in tutorial 2. Since tutorial 2 was easier, though, I thought it would be best to explain it before this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import DataSTORM.processors as proc\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of steps\n",
    "1. Search the parent directory for all localization files and a build a list of such files\n",
    "2. Define the drift correction processor\n",
    "3. Open a file from the list\n",
    "4. Perform the drift correction on the file\n",
    "  * If the correction was not good, go back to step 2\n",
    "4. Save the results\n",
    "5. Repeat from step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search the parent directory and make a list of localization files\n",
    "We will start by searching a parent directory and its subdirectories for all the localization files, i.e. those files that end in `locResults.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "parentDirectory   = Path('../test-data/Centrioles/')\n",
    "localizationFiles = parentDirectory.glob('**/*locResults.dat')\n",
    "locResultFiles    = sorted(localizationFiles)\n",
    "\n",
    "# How many files are there?\n",
    "print(len(locResultFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`locResultFiles` is an array of files ending in `locResults.dat` within the parent directory `../test-data/Centrioles/` and its subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../test-data/Centrioles/FOV_1_noPB_1500mW_10ms_1/FOV_1_noPB_1500mW_10ms_1_MMStack_locResults.dat')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locResultFiles[0] # In Python, array indexes start at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../test-data/Centrioles/FOV_7_noPB_1500mW_10ms_1/FOV_7_noPB_1500mW_10ms_1_MMStack_locResults.dat')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locResultFiles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the drift correction processor\n",
    "This part is the same as in Tutorial 1. At the end of processing for each file, return to this code block and start running the blocks again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the FiducialDriftCorrect processor and set its properties.\n",
    "dc = proc.FiducialDriftCorrect(minFracFiducialLength = 0.5,\n",
    "                               neighborRadius        = 500,\n",
    "                               smoothingWindowSize   = 2000,\n",
    "                               smoothingFilterSize   = 500,\n",
    "                               interactiveSearch     = True,\n",
    "                               noLinking             = True,\n",
    "                               noClustering          = True)\n",
    "\n",
    "clean = proc.CleanUp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a file from the list\n",
    "Now, we're going to open the first file in this list, locResultFiles[0]. When we are all done with this file, we will come back to here and increase the index by one (to locResultFiles[1], and so on...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the input and output files\n",
    "currentFile = locResultFiles[0] # Increment this when done dedrifting a dataset\n",
    "outputFile  = currentFile.parent / Path(currentFile.stem + '_DC' + currentFile.suffix)\n",
    "\n",
    "# Open a file and clean it up\n",
    "with open(str(currentFile), 'r') as file:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Clean up the data\n",
    "df = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x [nm]</th>\n",
       "      <th>y [nm]</th>\n",
       "      <th>z [nm]</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty [nm]</th>\n",
       "      <th>intensity [photon]</th>\n",
       "      <th>offset [photon]</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma [nm]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24970.090713</td>\n",
       "      <td>31160.454307</td>\n",
       "      <td>0</td>\n",
       "      <td>29187.099778</td>\n",
       "      <td>766.523967</td>\n",
       "      <td>4139.249990</td>\n",
       "      <td>186.332021</td>\n",
       "      <td>228.356764</td>\n",
       "      <td>138.228309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16545.141548</td>\n",
       "      <td>18919.796124</td>\n",
       "      <td>0</td>\n",
       "      <td>24342.325833</td>\n",
       "      <td>10230.296829</td>\n",
       "      <td>2868.737803</td>\n",
       "      <td>38.787012</td>\n",
       "      <td>436.768610</td>\n",
       "      <td>23.849260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>85.238000</td>\n",
       "      <td>0.106440</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.632450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.953000</td>\n",
       "      <td>-37.829000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9251.300000</td>\n",
       "      <td>11961.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6803.000000</td>\n",
       "      <td>3.752500</td>\n",
       "      <td>2290.300000</td>\n",
       "      <td>167.600000</td>\n",
       "      <td>86.804000</td>\n",
       "      <td>127.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24876.000000</td>\n",
       "      <td>27527.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>23345.000000</td>\n",
       "      <td>5.152900</td>\n",
       "      <td>3261.100000</td>\n",
       "      <td>179.600000</td>\n",
       "      <td>118.520000</td>\n",
       "      <td>134.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36370.000000</td>\n",
       "      <td>51665.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>49176.000000</td>\n",
       "      <td>6.637600</td>\n",
       "      <td>5090.200000</td>\n",
       "      <td>193.180000</td>\n",
       "      <td>198.420000</td>\n",
       "      <td>143.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65011.000000</td>\n",
       "      <td>65004.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>79999.000000</td>\n",
       "      <td>172430.000000</td>\n",
       "      <td>67290.000000</td>\n",
       "      <td>1315.400000</td>\n",
       "      <td>33596.000000</td>\n",
       "      <td>378.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x [nm]          y [nm]   z [nm]           frame  \\\n",
       "count  1120018.000000  1120018.000000  1120018  1120018.000000   \n",
       "mean     24970.090713    31160.454307        0    29187.099778   \n",
       "std      16545.141548    18919.796124        0    24342.325833   \n",
       "min         85.238000        0.106440        0      100.000000   \n",
       "25%       9251.300000    11961.000000        0     6803.000000   \n",
       "50%      24876.000000    27527.000000        0    23345.000000   \n",
       "75%      36370.000000    51665.000000        0    49176.000000   \n",
       "max      65011.000000    65004.000000        0    79999.000000   \n",
       "\n",
       "       uncertainty [nm]  intensity [photon]  offset [photon]   loglikelihood  \\\n",
       "count    1120018.000000      1120018.000000   1120018.000000  1120018.000000   \n",
       "mean         766.523967         4139.249990       186.332021      228.356764   \n",
       "std        10230.296829         2868.737803        38.787012      436.768610   \n",
       "min            0.632450            1.000000        68.953000      -37.829000   \n",
       "25%            3.752500         2290.300000       167.600000       86.804000   \n",
       "50%            5.152900         3261.100000       179.600000      118.520000   \n",
       "75%            6.637600         5090.200000       193.180000      198.420000   \n",
       "max       172430.000000        67290.000000      1315.400000    33596.000000   \n",
       "\n",
       "           sigma [nm]  \n",
       "count  1120018.000000  \n",
       "mean       138.228309  \n",
       "std         23.849260  \n",
       "min         54.000000  \n",
       "25%        127.090000  \n",
       "50%        134.690000  \n",
       "75%        143.700000  \n",
       "max        378.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the drift correction\n",
    "Now that the file is open we can perform the drift correction in the same manner as on a typical file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fiducial(s) detected.\n",
      "Performing spline fits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/matplotlib/backend_bases.py:2435: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "corrdf = dc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc.plotFiducials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have to check whether the fiducial trajectories are good enough to give a spline fit with the `plotFicuials()` function above. If they are not, you can cycle back to the definition of the corrector, change some of the parameters, and then rerun the steps.\n",
    "\n",
    "The general workflow for tuning the drift correction module is:\n",
    "\n",
    "1. Try to identify long-lived fiducials without using linking or clustering.\n",
    "2. If the fiducial tracks are noisy, enable linking and tune the linking parameters.\n",
    "3. If the fiducial tracks are still too noisy, enable clustering and tune the clustering parameters. (But only if there are fewer than 50,000 frames! Otherwise, clustering will eat up all the memory in the computer.)\n",
    "4. If you can't get any good fiducials, reload the DataFrame and skip the drift correction steps. Alternatively, if no fiducials were officially found in the linking/clustering, run the code below to save the file with a _DCX suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(dc.fiducialTrajectories) == 0:\n",
    "    with open(str(currentFile), 'r') as file:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "    df = clean(df)\n",
    "\n",
    "    outputFile = currentFile.parent / Path(currentFile.stem + '_DCX' + currentFile.suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open(str(outputFile), 'w') as file:\n",
    "    df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can cycle back up to the beginning and increase the index locResultFiles array by one and repeat the process in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
