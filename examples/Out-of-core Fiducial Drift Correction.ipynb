{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the localization files that are generated by Fang's software are 4 GB in size or more. Reading in this data and processing it poses a problem because of its large size and the limited amount of memory available on our machines. A solution to this problem is to employ out-of-core processing (also known as chunking), whereby data is broken up into tiny bits, processed, and then written to disk before more data is read into memory.\n",
    "\n",
    "Python's Pandas library is already well-suited to employ out-of-core processing because it is built into the library. However, to use it with DataSTORM's FiducialDriftCorrection processor, we need to manually drive the FiducialDriftCorrection methods that it would otherwise automatically perform.\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to perform out-of-core processing on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary libraries\n",
    "%pylab\n",
    "import DataSTORM.processors as ds\n",
    "import pandas               as pd\n",
    "import importlib\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by opening a connection to the file so that we may pull out subsets of data at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filePath      = Path('../test-data/MicroTubules_LargeFOV/FOV2_1500_10ms_1_MMStack_locResults.dat')\n",
    "numRowsToRead = 8e6 # Read 8 million rows at a time. This should be as large as is reasonable.\n",
    "\n",
    "# Opens a connection to the file, reading in 200000 rows at a time.\n",
    "reader        = pd.read_csv(str(filePath.resolve()), chunksize = numRowsToRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reader* is a TextFileReader object that may be iterated over to extract the data. Alternatively, we may use its *.get_chunk()* method to extract a chunk with a specific number of rows. Note that everytime *.get_chunk(size)* is called, a pointer to the current row inside the file moves forward by *size* rows.\n",
    "\n",
    "The pointer can not be moved backwards, so once *.get_chunk()* is called, you must do something with those rows. Otherwise, you will need to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you uncomment the next line to try .get_chunk(), you should restart the notebook.\n",
    "#reader.get_chunk(50000).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have opened a connection to the file, let's begin the dedrift process by interactively searching for fiducials in each chunk. The outline of the steps looks like this:\n",
    "\n",
    "1. Create a FiducialDriftCorrect processor from DataSTORM. Note that in normal (on-core) processing we set its *interactiveSearch* flag to True so that the interactive search for fiducials is performed automatically. Here, since we'll direct the search process on each chunk, we will leave it at its default value of **False**.\n",
    "\n",
    "2. Loop through each chunk. For each chunk allow the user to specify a subregion containing fidcuials. The processor will remember each subregion that the user specified.\n",
    "\n",
    "3. Within the same loop iteration, filter out localizations from the current chunk that do not lie within the search areas. Append these localizations to a DataFrame that is collecting all localizations that are fiducial candidates.\n",
    "\n",
    "4. Use the *detectFiducials()* method of the processor class to look for fiducial trajectories within the localizations that are output from step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the FiducialDriftCorrect processor.\n",
    "dc = ds.FiducialDriftCorrect(mergeRadius           = 50,\n",
    "                             offTime               = 1,\n",
    "                             minSegmentLength      = 20,\n",
    "                             minFracFiducialLength = 0.4,\n",
    "                             neighborRadius        = 500,\n",
    "                             smoothingWindowSize   = 500,\n",
    "                             smoothingFilterSize   = 300)\n",
    "\n",
    "# Create a CleanUp processor to ensure data in each chunk is clean.\n",
    "clean = ds.CleanUp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you select a region containing a fiducial in chunk, that region is remembered for all frames. So, if you have already selected a region containing a fiducial in one chunk, you do not have to select the same region again in other chunks unless you believe that the fiducial has drifted out of the region you selected in earlier chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/ipykernel/ipkernel.py:175: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/matplotlib/backend_bases.py:2435: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "# fids will hold the localizations belonging to fiducials and will grow with each processed chunk\n",
    "fids = pd.DataFrame()\n",
    "\n",
    "# minFrame and maxFrame will hold the absolute min and maximum frame in all the chunks\n",
    "minFrame = 0\n",
    "maxFrame = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    # Clean up the data in the chunks\n",
    "    chunk = clean(chunk)\n",
    "    \n",
    "    # Update the minimum and maximum frames with each chunk\n",
    "    minFrame = np.min([minFrame, chunk['frame'].min()])\n",
    "    maxFrame = np.max([maxFrame, chunk['frame'].max()])\n",
    "    \n",
    "    # Rename the columns because trackpy does not accept column names with units\n",
    "    chunk.rename(columns = {'x [nm]' : 'x', 'y [nm]' : 'y'}, inplace = True)\n",
    "    \n",
    "    # It's important to set resetRegions = False here.\n",
    "    # Otherwise, the regions will be overwritten for each new chunk.\n",
    "    dc.iSearch(chunk, resetRegions = False)\n",
    "    \n",
    "    # Filter out localizations that are not within all previously-defined search regions.\n",
    "    # Append these to the fiducial Data Frame defined just before the start of the loop.\n",
    "    currentFids = dc.reduceSearchArea(chunk)\n",
    "    fids        = fids.append(currentFids, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z [nm]</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty [nm]</th>\n",
       "      <th>intensity [photon]</th>\n",
       "      <th>offset [photon]</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma [nm]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269</td>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269.000000</td>\n",
       "      <td>72269.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37172.145429</td>\n",
       "      <td>22195.987848</td>\n",
       "      <td>0</td>\n",
       "      <td>25429.689203</td>\n",
       "      <td>6.130453</td>\n",
       "      <td>3749.025102</td>\n",
       "      <td>299.799022</td>\n",
       "      <td>228.021819</td>\n",
       "      <td>128.672439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16696.546577</td>\n",
       "      <td>19424.913094</td>\n",
       "      <td>0</td>\n",
       "      <td>16136.743263</td>\n",
       "      <td>2.321445</td>\n",
       "      <td>2508.708965</td>\n",
       "      <td>50.801959</td>\n",
       "      <td>1162.593001</td>\n",
       "      <td>17.978308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20606.000000</td>\n",
       "      <td>1935.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.576660</td>\n",
       "      <td>738.220000</td>\n",
       "      <td>98.010000</td>\n",
       "      <td>30.063000</td>\n",
       "      <td>78.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20862.000000</td>\n",
       "      <td>2272.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>9484.000000</td>\n",
       "      <td>3.910800</td>\n",
       "      <td>2087.400000</td>\n",
       "      <td>267.530000</td>\n",
       "      <td>95.100000</td>\n",
       "      <td>113.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21055.000000</td>\n",
       "      <td>41049.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>26816.000000</td>\n",
       "      <td>6.294600</td>\n",
       "      <td>2845.500000</td>\n",
       "      <td>287.140000</td>\n",
       "      <td>138.240000</td>\n",
       "      <td>128.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54268.000000</td>\n",
       "      <td>41132.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>40193.000000</td>\n",
       "      <td>8.141800</td>\n",
       "      <td>5354.100000</td>\n",
       "      <td>325.650000</td>\n",
       "      <td>219.860000</td>\n",
       "      <td>143.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>54695.000000</td>\n",
       "      <td>41620.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>14.287000</td>\n",
       "      <td>75098.000000</td>\n",
       "      <td>1863.300000</td>\n",
       "      <td>267490.000000</td>\n",
       "      <td>249.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y  z [nm]         frame  uncertainty [nm]  \\\n",
       "count  72269.000000  72269.000000   72269  72269.000000      72269.000000   \n",
       "mean   37172.145429  22195.987848       0  25429.689203          6.130453   \n",
       "std    16696.546577  19424.913094       0  16136.743263          2.321445   \n",
       "min    20606.000000   1935.900000       0    100.000000          0.576660   \n",
       "25%    20862.000000   2272.600000       0   9484.000000          3.910800   \n",
       "50%    21055.000000  41049.000000       0  26816.000000          6.294600   \n",
       "75%    54268.000000  41132.000000       0  40193.000000          8.141800   \n",
       "max    54695.000000  41620.000000       0  49999.000000         14.287000   \n",
       "\n",
       "       intensity [photon]  offset [photon]  loglikelihood    sigma [nm]  \n",
       "count        72269.000000     72269.000000   72269.000000  72269.000000  \n",
       "mean          3749.025102       299.799022     228.021819    128.672439  \n",
       "std           2508.708965        50.801959    1162.593001     17.978308  \n",
       "min            738.220000        98.010000      30.063000     78.444000  \n",
       "25%           2087.400000       267.530000      95.100000    113.350000  \n",
       "50%           2845.500000       287.140000     138.240000    128.490000  \n",
       "75%           5354.100000       325.650000     219.860000    143.060000  \n",
       "max          75098.000000      1863.300000  267490.000000    249.810000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fids.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 49999: 2 trajectories present\n",
      "2 fiducial(s) detected.\n"
     ]
    }
   ],
   "source": [
    "# Detect the fiducuial trajectories from these localizations\n",
    "dc.detectFiducials(fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop fiducials from the list of localizations\n",
    "dc.fitSplines()\n",
    "dc.combineSplines(None, startFrame = minFrame, stopFrame = maxFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were detected fiducials and everything went well, we can now plot the fiducial tracks and the average spline to verify that we have a good drift correction curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc.plotFiducials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the actual drift correction\n",
    "Now that we have the drift curves, we need to perform the actual correction. This is achieved by chunking the same file as before and dynamically writing the corrected data to another file while each chunk is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../test-data/MicroTubules_LargeFOVFOV2_1500_10ms_1_MMStack_locResults_DC.dat')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(str(filePath.parent) + filePath.stem + '_DC' + filePath.suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile = Path(str(filePath.parent) + filePath.stem + '_DC' + filePath.suffix)\n",
    "\n",
    "reader = pd.read_csv(str(filePath.resolve()), chunksize = 1e6) # We'll read fewer rows this time\n",
    "for chunk in reader:\n",
    "    chunk = dc.dropFiducials()\n",
    "    chunk = dc._correctLocalizations(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
