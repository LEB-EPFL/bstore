{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook demonstrates how a normal processing pipleline works without batch processing. The outline of processing steps are as follows:\n",
    "\n",
    "1. Clean up the data\n",
    "2. Perform drift correction\n",
    "3. Apply light filtering to the data to prepare for merging\n",
    "4. Merge localizations into one\n",
    "5. Apply any final filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "from DataSTORM import processors as proc\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in a file\n",
    "Python's *pathlib* library makes reading files easy inside Jupyter Notebooks because of its tab-completion feature. Simply start typing the directory inside the Path object, press TAB, and it will list files and folders in the current directory. Use `..` to go up one directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Press TAB inside the quotation marks\n",
    "filePath = Path('../test-data/Centrioles/FOV_1_noPB_1500mW_10ms_1/FOV_1_noPB_1500mW_10ms_1_MMStack_locResults.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the data into a DataFrame. A DataFrame is container for data from the Pandas library that essentially acts as a spreadsheet. It's very fast and supports loading from multiple file formats. It also supports out-of-core processing by loading chunks of data into memory at time. We will use its simplest method with standard arguments, `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# str() converts the Path to a string\n",
    "# 'r' means to open the file in read-mode\n",
    "# df holds the DataFrame returned from pd.read_csv()\n",
    "with open(str(filePath), 'r') as file:\n",
    "    df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a summary of the data, you can use the `describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x [nm]</th>\n",
       "      <th>y [nm]</th>\n",
       "      <th>z [nm]</th>\n",
       "      <th>frame</th>\n",
       "      <th>uncertainty [nm]</th>\n",
       "      <th>intensity [photon]</th>\n",
       "      <th>offset [photon]</th>\n",
       "      <th>loglikelihood</th>\n",
       "      <th>sigma [nm]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1128672.000000</td>\n",
       "      <td>1128672.000000</td>\n",
       "      <td>1128672</td>\n",
       "      <td>1128672.000000</td>\n",
       "      <td>1120018.000000</td>\n",
       "      <td>1128672.000000</td>\n",
       "      <td>1128672.000000</td>\n",
       "      <td>1128672.000000</td>\n",
       "      <td>1128672.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25114.914616</td>\n",
       "      <td>31378.310512</td>\n",
       "      <td>0</td>\n",
       "      <td>29113.635001</td>\n",
       "      <td>766.523967</td>\n",
       "      <td>4130.626808</td>\n",
       "      <td>186.466802</td>\n",
       "      <td>235.056396</td>\n",
       "      <td>139.046655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16600.385105</td>\n",
       "      <td>19042.742324</td>\n",
       "      <td>0</td>\n",
       "      <td>24341.727052</td>\n",
       "      <td>10230.296829</td>\n",
       "      <td>2859.651891</td>\n",
       "      <td>38.698657</td>\n",
       "      <td>443.057482</td>\n",
       "      <td>25.664258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>85.238000</td>\n",
       "      <td>0.106440</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.632450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.953000</td>\n",
       "      <td>-37.829000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9274.200000</td>\n",
       "      <td>12213.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6756.000000</td>\n",
       "      <td>3.752500</td>\n",
       "      <td>2296.400000</td>\n",
       "      <td>167.700000</td>\n",
       "      <td>87.016000</td>\n",
       "      <td>127.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24997.000000</td>\n",
       "      <td>27570.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>23178.000000</td>\n",
       "      <td>5.152900</td>\n",
       "      <td>3252.700000</td>\n",
       "      <td>179.740000</td>\n",
       "      <td>119.160000</td>\n",
       "      <td>134.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36672.000000</td>\n",
       "      <td>52188.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>49093.000000</td>\n",
       "      <td>6.637600</td>\n",
       "      <td>5070.900000</td>\n",
       "      <td>193.480000</td>\n",
       "      <td>202.190000</td>\n",
       "      <td>143.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65011.000000</td>\n",
       "      <td>65004.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>79999.000000</td>\n",
       "      <td>172430.000000</td>\n",
       "      <td>67290.000000</td>\n",
       "      <td>1315.400000</td>\n",
       "      <td>33596.000000</td>\n",
       "      <td>378.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x [nm]          y [nm]   z [nm]           frame  \\\n",
       "count  1128672.000000  1128672.000000  1128672  1128672.000000   \n",
       "mean     25114.914616    31378.310512        0    29113.635001   \n",
       "std      16600.385105    19042.742324        0    24341.727052   \n",
       "min         85.238000        0.106440        0      100.000000   \n",
       "25%       9274.200000    12213.000000        0     6756.000000   \n",
       "50%      24997.000000    27570.000000        0    23178.000000   \n",
       "75%      36672.000000    52188.000000        0    49093.000000   \n",
       "max      65011.000000    65004.000000        0    79999.000000   \n",
       "\n",
       "       uncertainty [nm]  intensity [photon]  offset [photon]   loglikelihood  \\\n",
       "count    1120018.000000      1128672.000000   1128672.000000  1128672.000000   \n",
       "mean         766.523967         4130.626808       186.466802      235.056396   \n",
       "std        10230.296829         2859.651891        38.698657      443.057482   \n",
       "min            0.632450            1.000000        68.953000      -37.829000   \n",
       "25%            3.752500         2296.400000       167.700000       87.016000   \n",
       "50%            5.152900         3252.700000       179.740000      119.160000   \n",
       "75%            6.637600         5070.900000       193.480000      202.190000   \n",
       "max       172430.000000        67290.000000      1315.400000    33596.000000   \n",
       "\n",
       "           sigma [nm]  \n",
       "count  1128672.000000  \n",
       "mean       139.046655  \n",
       "std         25.664258  \n",
       "min         54.000000  \n",
       "25%        127.150000  \n",
       "50%        134.800000  \n",
       "75%        143.990000  \n",
       "max        378.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the data\n",
    "Many data files might have a few rows that contain NaN's, Inf's, or incorrectly formatted data. DataSTORM provides a processor called CleanUp that fixes these. It is not necessary to clean up the data in this example, but we will do it anyway to demonstrate how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaner = proc.CleanUp()\n",
    "df      = cleaner(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform fiducial-based drift correction\n",
    "The most work is in performing fiducial based drift correction because of the large number of parameters you can tune. If your fiducials are present in nearly every frame and highly visible, the easiest option is to simply set the interactive search to True and skip linking and spatial clustering.\n",
    "\n",
    "To see all options you can set, press `SHIFT-TAB` twice with the cursor just after the first paranthesis of FiducialDriftCorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrector = proc.FiducialDriftCorrect(minFracFiducialLength = 0.75, # Fiducials must span 75% of number of frames\n",
    "                                      interactiveSearch     = True, # Select fiducials by eye\n",
    "                                      noLinking             = True, # Do not perform Crocker-Grier linking\n",
    "                                      noClustering          = True) # Do not spatially cluster fiducials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the corrector is run, it will display a 2D histogram image. You may zoom in and out of regions and draw a rectangle around areas with large counts. Areas with counts that are approximately equal to the number of frames are likely to be fiducials. There is a fiducial in this dataset in three bins around (x = 28, y = 55.5).\n",
    "\n",
    "With the selection rectangle around a region, press `SPACE` to add the region to the list of areas to search for fiducials. Press `r` if you want to reset the regions to empty. When you are done, simply close the window.\n",
    "\n",
    "If no region is selected, the fiducial search will be performed over the whole set of localizations, which can either be slow or lead to completely wrong results if linking and clustering are turned off.\n",
    "\n",
    "Also note that the corrector removes fiducials, so it is best to save the output to another DataFrame, in this case `corrDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fiducial(s) detected.\n",
      "Performing spline fits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/matplotlib/backend_bases.py:2435: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "corrDF = corrector(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the quality of the drift correction curves using `plotFiducials()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrector.plotFiducials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not necessary for this dataset, but if the drift correction could be improved, we can adjust some of the smoothing parameters and rerun the drift correction. For this example, we'll turn on linking and throw out trajectories shorter than ten consecutive frames. Additionally, we'll shrink the size of the smoothing window and filters to better capture changes in the fiducial trajectory. There are a few parameters for linking and clustering, but we'll leave them at their defaults.\n",
    "\n",
    "Note that clustering the fiducials can often help get rid of noisy points. However, DBSCAN breaks down if the fiducials are more than about 50,000 frames, so it is preferable to turn it off if you have a long fiducial track as in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrector = proc.FiducialDriftCorrect(minFracFiducialLength = 0.75,   # Fiducials must span 75% of number of frames\n",
    "                                      interactiveSearch     = True,   # Select fiducials bye eye\n",
    "                                      noLinking             = False,  # Perform Crocker-Grier linking\n",
    "                                      noClustering          = True,   # Perform DBSCAN to cluster fiducials\n",
    "                                      smoothingWindowSize   = 750,    # Set the moving window size for smoothing\n",
    "                                      smoothingFilterSize   = 100)    # Set Gaussian filter std. dev. for smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 79999: 1 trajectories present\n",
      "1 fiducial(s) detected.\n",
      "Performing spline fits...\n"
     ]
    }
   ],
   "source": [
    "corrDF = corrector(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrector.plotFiducials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now investigate the corrected localizations. The x and y columns now contain the corrected localizations. `dx` and `dy` contain the amount of the correction. To get the original data back, one can simply add `dx` to `x` and the same for `y`.\n",
    "\n",
    "Note that the new count is less than the original one. This is because the drift correction removed localizations belonging to the fiducial marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the data\n",
    "At this point, we can now filter the data by setting criteria on the columns. First we define the filters. After that, we simply apply them in reverse order to the DataFrame to get the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter1 = proc.Filter('sigma [nm]', '<', 200)\n",
    "filter2 = proc.Filter('sigma [nm]', '>', 100)\n",
    "\n",
    "fcDF = filter2(filter1(corrDF)) # First filter1 is applied, then filter2 is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fcDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging localizations\n",
    "The last step in the analysis pipeline typically involves merging localizations that are on for several frames into one. This is performed by the Crocker-Grier algorithm in trackpy, but all you have to worry about it defining a Merge processor and applying it to the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merger = proc.Merge(tOff            = 1,  # Number of frames that a molecule can be missing and still be part of a track\n",
    "                    mergeRadius     = 40) # Maximum distance between successive molecules\n",
    "\n",
    "mfcDF = merger(fcDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfcDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final filtering and saving\n",
    "At this point, the data may be filtered once more in the same manner as above. Let's skip this part and save the data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputFile = filePath.parent /  Path(filePath.stem + '_DC_Merged' + filePath.suffix)\n",
    "print(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(str(outputFile), 'w') as file:\n",
    "    mfcDF.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
