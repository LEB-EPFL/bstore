{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data in batch from a datastore\n",
    "A major advantage of the automatic organization of data in B-Store is that batch processing becomes very simple. Batch processing is the automated processing and analysis of selected data from the datastore.\n",
    "\n",
    "A batch process typically goes as follows:\n",
    "1. Define a batch processor and tell it where your datastore is located\n",
    "2. Define a pipeline consisting of processors that perform operations on the data\n",
    "3. Run the process and output the datafiles to a directory on your computer for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the essential bstore libraries\n",
    "from bstore import processors, batch\n",
    "\n",
    "# This is part of Python 3.4 and greater and not part of B-Store\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting: Get the test data\n",
    "Once again, we will use data inside B-Store's test file repository in this example. Clone or download the repository from https://github.com/kmdouglass/bstore_test_files and point the variable below to *test_experiment/test_experiment_db.h5* within the *bstore_test_files* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbFile = Path('../../bstore_test_files/test_experiment/test_experiment_db.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step one: Define the processing pipeline\n",
    "A batch processor works by opening each dataset in a B-Store datastore and applying a `processor` to it in sequence. A processor represents one fundamental processing step that can be performed on a localization dataset.\n",
    "\n",
    "In this example, we'll simply filter out localizations that are poorly localized by pulling only rows from the datastore whose precision values are less than 20 and whose loglikelihood values are 250 or less. The pipeline will consist of a Python list of these two `Filter` processors. The order in which they are applied goes from the first element in the list to the last.\n",
    "\n",
    "## Current list of processors\n",
    "At the time of this writing, B-Store provides these built-in processors:\n",
    "\n",
    "1. **AddColumn** - Adds a single column to a DataFrame and fills it with a default value\n",
    "2. **CleanUp** - Removes rows containing invalid entries, such as `Inf` or `NaN`\n",
    "3. **Cluster** - Performs spatial clustering on localizations\n",
    "4. **ComputeClusterStats** - Computes features of clustered localizations\n",
    "5. **ConvertHeader** - Changes the names of the columns\n",
    "6. **FiducialDriftCorrect** - Interactively find fiducial beads to perform drift correction\n",
    "7. **Filter** - Filter out rows not matching the filter criteria\n",
    "8. **Merge** - Merge nearby localizations in time and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the processors\n",
    "# uncertainty and loglikelihood are column names\n",
    "Filter1 = processors.Filter('uncertainty',   '<',   20) # Note the quotation marks ''\n",
    "Filter2 = processors.Filter('loglikelihood', '<=', 250)\n",
    "\n",
    "# Create the pipeline; [...] denotes a Python list\n",
    "pipeline = [\n",
    "            Filter1,\n",
    "            Filter2\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step two: Setup the batch processor\n",
    "With the pipeline defined, we can now setup the batch processor. Since our datastore is inside an HDF file, we'll use B-Store's `HDFBatchProcessor` to read the data and apply the pipeline.\n",
    "\n",
    "When creating the `HDFBatchProcessor`, we need to supply two arguments:\n",
    "\n",
    "1. `dbFile` - Our B-Store HDF datastore file\n",
    "2. `pipeline` - The list of processors to apply to the data\n",
    "\n",
    "The optional arguments to `HDFBatchProcessor` are\n",
    "\n",
    "1. `outputDirectory` - The full path to a directory to output the results. By default, this is a folder in the same directory as the calling code and is called *processed_data*. If the `outputDirectory` does not exist, it will be automatically created.\n",
    "2. `searchString` - A string matching one of B-Store's dataset types and that identifies the type of data in the datastore to process. By default this is `locResults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp = batch.HDFBatchProcessor(dbFile, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialized, the batch processor will open the datastore and locate all the dataset types matching `searchString`. We can investigate the datasets it found through its `datasetList` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetID(prefix='HeLaL_Control', acqID=1, datasetType='Localizations', attributeOf=None, channelID='A647', dateID=None, posID=(0,), sliceID=None)\n",
      "DatasetID(prefix='HeLaS_Control', acqID=2, datasetType='Localizations', attributeOf=None, channelID='A647', dateID=None, posID=(0,), sliceID=None)\n"
     ]
    }
   ],
   "source": [
    "for ds in bp.datasetList:\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step three: Run the batch processor\n",
    "Now that the batch processor is setup, we use the `go()` method to automatically apply our pipeline to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory does not exist. Creating it...\n",
      "Created folder /home/kmdouglass/src/bstore/examples/processed_data\n"
     ]
    }
   ],
   "source": [
    "bp.go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step four: analyze the results\n",
    "The batch processor has output its results into the *processed_data* directory. Here's what this contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_data/:\r\n",
      "\u001b[0m\u001b[01;34mHeLaL_Control\u001b[0m/  \u001b[01;34mHeLaS_Control\u001b[0m/\r\n",
      "\r\n",
      "processed_data/HeLaL_Control:\r\n",
      "\u001b[01;34mHeLaL_Control_1\u001b[0m/\r\n",
      "\r\n",
      "processed_data/HeLaL_Control/HeLaL_Control_1:\r\n",
      "Localizations_ChannelA647_Pos0.csv  Localizations_ChannelA647_Pos0.json\r\n",
      "\r\n",
      "processed_data/HeLaS_Control:\r\n",
      "\u001b[01;34mHeLaS_Control_2\u001b[0m/\r\n",
      "\r\n",
      "processed_data/HeLaS_Control/HeLaS_Control_2:\r\n",
      "Localizations_ChannelA647_Pos0.csv  Localizations_ChannelA647_Pos0.json\r\n"
     ]
    }
   ],
   "source": [
    "# This is just a Linux command that prints\n",
    "# all files and folders in a directory structure\n",
    "%ls -R processed_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is telling us that a folder was generated for each dataset prefix in the datastore, in this case `HeLaL_Control` and `HeLaS_Control`. Inside each of these folders, another folder was generated for each specific acquisition, here `HeLaL_Control_1` and `HeLaS_Control_2`. Finally, these folders contain two files each. The processed localizations are in .csv files and can be opened in any software package that can process column separated values, like [ThunderSTORM](https://github.com/zitmen/thunderstorm) or even Microsoft Excel. Each .csv file has a corresponding .json file that contains the B-Store dataset ID's, ensuring that each processed dataset can be traced back to its original dataset in the datastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra note: batch processing with Python set operations\n",
    "\n",
    "As noted in [Tutorial 1](https://github.com/kmdouglass/bstore/blob/master/examples/Tutorial%201%20-%20Introduction%20to%20B-Store%20databases.ipynb), you can iterate over Datastores using standard Python set operations. This means that, strictly speaking, the `HDFBatchProcessor` is not entirely necessary. It is provided, however, as a convenience to those not familiar with Python.\n",
    "\n",
    "If however you are familiar with functional programming styles in Python, you can chain processors together since the output of one can be sent as an argument to the next. These processors can be applied to each dataset by iterating over the Datastore and saving the results if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra note: batch processing on CSV files\n",
    "In addition to the `HDFBatchProcessor`, B-Store provides a `CSVBatchProcessor` for performing a batch process on .csv files containing localization data. This is useful if you have already pulled data out of a datastore and processed it once, but want to perform additional steps on the processed data.\n",
    "\n",
    "The `CSVBatchProcessor` works in much the same way as the `HDFBatchProcessor`. Instead of searching a datastore, it searches a directory and sub-directories for all files matching a pattern in its `suffix` argument. The processed files will be placed in the directory contained in the `outputDirectory` argument if `useSameFolder` is set to False. If `useSameFolder = True`, the additionally processed files will be located in the same folder as the originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an additional filter to apply to the\n",
    "# already processed results\n",
    "newPipeline = [processors.Filter('sigma', '<', 175)]\n",
    "\n",
    "# Search for all .csv files in the processed_data\n",
    "# directory\n",
    "inputDirectory = Path('processed_data/')\n",
    "suffix         = '.csv'\n",
    "\n",
    "# Create the CSV batch processor\n",
    "bpCSV = batch.CSVBatchProcessor(inputDirectory, newPipeline,\n",
    "                                useSameFolder = True, suffix = suffix)\n",
    "\n",
    "# Run the CSV batch processor\n",
    "bpCSV.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_data/:\r\n",
      "\u001b[0m\u001b[01;34mHeLaL_Control\u001b[0m/  \u001b[01;34mHeLaS_Control\u001b[0m/\r\n",
      "\r\n",
      "processed_data/HeLaL_Control:\r\n",
      "\u001b[01;34mHeLaL_Control_1\u001b[0m/\r\n",
      "\r\n",
      "processed_data/HeLaL_Control/HeLaL_Control_1:\r\n",
      "Localizations_ChannelA647_Pos0.csv\r\n",
      "Localizations_ChannelA647_Pos0.json\r\n",
      "Localizations_ChannelA647_Pos0_processed.csv\r\n",
      "\r\n",
      "processed_data/HeLaS_Control:\r\n",
      "\u001b[01;34mHeLaS_Control_2\u001b[0m/\r\n",
      "\r\n",
      "processed_data/HeLaS_Control/HeLaS_Control_2:\r\n",
      "Localizations_ChannelA647_Pos0.csv\r\n",
      "Localizations_ChannelA647_Pos0.json\r\n",
      "Localizations_ChannelA647_Pos0_processed.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Display the new contents of processed_data\n",
    "%ls -R processed_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that there are two additional files ending in \\*processed.csv and containing the additionally processed localizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "1. Localization data inside a B-Store HDF datastore can be automatically processed using a `HDFBatchProcessor`.\n",
    "2. A batch process consists of a list of processors (known as a pipeline) that are sequentially applied to each dataset in the datastore.\n",
    "3. B-Store comes with a few processors already for performing common computations on the localization data.\n",
    "4. A batch processor automatically applies the pipeline to the data and saves the results in a structured output directory for further analysis. To do this, call the `go()` method.\n",
    "5. B-Store supplies a `CSVBatchProcessor` for automatically processing .csv files in multiple directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up the example files\n",
    "import shutil\n",
    "shutil.rmtree('processed_data/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
