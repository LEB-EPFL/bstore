{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple database from a small experiment\n",
    "In this tutorial we'll take a small experiment which includes raw localizations, widefield images, and metadata and build them into a database. The database will exist within an [HDF](https://www.hdfgroup.org/) file. The organization of the data inside the file will be handled by B-Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the essential bstore libraries\n",
    "from bstore import database, parsers\n",
    "\n",
    "# This is part of Python 3.4 and greater and not part of B-Store\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting: Get the test data\n",
    "You can get the test data for this tutorial from the B-Store test repository at https://github.com/kmdouglass/bstore_test_files. Clone or download the files and change the filename below to point to the folder *test_experiment_2* within this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDirectory = Path('../../bstore_test_files/test_experiment_2/') # ../ means go up one directory level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step one: Create a parser to read the datasets\n",
    "In this step, we'll create a parser that can read the files that are stored inside the test data directory. The default parser that comes with B-Store is called `MMParser` and is short for Micro-Manager parser. This is the parser that we use to read datasets that were generated by Micro-Manager and our own localization computing software.\n",
    "\n",
    "In a later tutorial, we'll show you how to write a simple parser to parse your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the parser\n",
    "parser = parsers.MMParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Of course, this step is easy if a parser already exists for your data.\n",
    "\n",
    "We're also ignorning some optional arguments inside the `MMParser()` constructor, but we'll get to those in a later tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step two: Create the empty database object\n",
    "The database object is what Python uses to build a database inside an HDF file. When we create the object, we specify a path to the file where the information will be stored.\n",
    "\n",
    "Note that no file is created until data is actually put into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The path is relative to this notebook.\n",
    "# Altnernatively, you could send a Path object to HDFDatabase constructor.\n",
    "dbName = 'myFirstDatabase.h5'\n",
    "myDB   = database.HDFDatabase(dbName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step three: Build the database\n",
    "Now comes the fun part. We build the database by using the HDFDatabase's `build()` method. To do this, we need to send a few required arguments to the method. These are:\n",
    "\n",
    "1. parser - The parser to use when interpreting the data files\n",
    "2. searchDirectory - The parent directory containing subdirectories with all the experimental data\n",
    "\n",
    "There are also a few optional arguments whose defaults we will override to match our data files naming patterns. These optional arguments are\n",
    "\n",
    "1. locResultsString - A string at the end of all raw localization file names, including the file type\n",
    "2. locMetadataString - Same as above, but for metadata associated with the localization files\n",
    "3. widefieldImageString - A string at the end of of the file names of any widefield images in the directory\n",
    "\n",
    "Finally, there is a boolean argument named `dryRun`. If you set this to True, the build method won't actually create the database. It will however return a structure that tells you what datasets were successfully parsed and capable of insertion into the database. By default, `dryRun` is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Make column names in the test dataset without spaces and use those as the test data\n",
    "# TODO: Do a dry run build\n",
    "#myDB.build()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
