{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the localization files that are generated by Fang's software are 4 GB in size or more. Reading in this data and processing it poses a problem because of its large size and the limited amount of memory available on our machines. A solution to this problem is to employ out-of-core processing (also known as chunking), whereby data is broken up into tiny bits, processed, and then written to disk before more data is read into memory.\n",
    "\n",
    "Python's Pandas library is already well-suited to employ out-of-core processing because it is built into the library. However, to use it with DataSTORM's FiducialDriftCorrection processor, we need to manually drive the FiducialDriftCorrection methods that it would otherwise automatically perform.\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to perform out-of-core processing on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary libraries\n",
    "%pylab\n",
    "import DataSTORM.processors as ds\n",
    "import pandas               as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input file\n",
    "filePath   = Path('../test-data/MicroTubules_LargeFOV/FOV2_1500_10ms_1_MMStack_locResults.dat')\n",
    "\n",
    "# Output file\n",
    "outputFile = filePath.parent / Path(filePath.stem + '_DC' + filePath.suffix)\n",
    "\n",
    "\n",
    "numRowsToReadInteractive = 8e6 # Read 8 million rows when searching for fiducials.\n",
    "numRowsToRead            = 1e6 # Number of rows to read for all other operations.\n",
    "\n",
    "# Create the FiducialDriftCorrect processor and set its properties.\n",
    "dc = ds.FiducialDriftCorrect(mergeRadius           = 50,\n",
    "                             offTime               = 1,\n",
    "                             minSegmentLength      = 20,\n",
    "                             minFracFiducialLength = 0.4,\n",
    "                             neighborRadius        = 500,\n",
    "                             smoothingWindowSize   = 500,\n",
    "                             smoothingFilterSize   = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a file for out-of-core processing\n",
    "We will start by opening a connection to the file so that we may pull out subsets of data at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opens a connection to the file, reading in 200000 rows at a time.\n",
    "reader        = pd.read_csv(str(filePath.resolve()), chunksize = numRowsToReadInteractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reader* is a TextFileReader object that may be iterated over to extract the data. Alternatively, we may use its *.get_chunk()* method to extract a chunk with a specific number of rows. Note that everytime *.get_chunk(size)* is called, a pointer to the current row inside the file moves forward by *size* rows.\n",
    "\n",
    "## Overview of the fiducial-based drift correction using OOC processing\n",
    "Now that we have opened a connection to the file, let's begin the dedrift process by interactively searching for fiducials in each chunk. The outline of the steps looks like this:\n",
    "\n",
    "1. Create a FiducialDriftCorrect processor from DataSTORM. Note that in normal (on-core) processing we set its *interactiveSearch* flag to True so that the interactive search for fiducials is performed automatically. Here, since we'll direct the search process on each chunk, we will leave it at its default value of **False**.\n",
    "\n",
    "2. Loop through each chunk of data. For each chunk allow the user to specify a subregion containing fiducials. The processor will remember each subregion that the user specified. This step is best performed on chunks as large as possible because the fiducials may drift between chunks.\n",
    "\n",
    "3. Loop over all chunks again, filtering out localizations from the current chunk that do not lie within the search areas. Append these localizations to a DataFrame that is collecting all localizations that are fiducial candidates. This DataFrame will be fed to the FiducialDriftCorrect processor's regular routines to build the correction curve.\n",
    "\n",
    "4. Use the *detectFiducials()* method of the processor class to look for fiducial trajectories within the localizations that are output from step 3.\n",
    "\n",
    "5. Compute the fiducial drift correction curves from the identified fiducials.\n",
    "\n",
    "6. Open the data in chunks one last time, applying the correction to each localization and stream the data to a different file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactively select regions containing fiducials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a CleanUp processor to ensure data in each chunk is clean.\n",
    "clean = ds.CleanUp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you select a region containing a fiducial in chunk, that region is remembered for all frames. So, if you have already selected a region containing a fiducial in one chunk, you do not have to select the same region again in other chunks unless you believe that the fiducial has drifted out of the region you selected in earlier chunks.\n",
    "\n",
    "**Reminder**: Use the zoom tool in the figure to zoom in on regions containing high counts. Deactivate the zoom tool and then click and drag around a bin with a large count, making sure the border of the selection rectangle is just a tiny bit bigger than the bin. Press space to add a region to the list of regions to search for fiducials. You typically will not need more than three fiducials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/ipykernel/ipkernel.py:175: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/matplotlib/backend_bases.py:2435: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "# minFrame and maxFrame will hold the absolute min and maximum frame in all the chunks\n",
    "minFrame = 1e7\n",
    "maxFrame = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    # Clean up the data in the chunks\n",
    "    chunk = clean(chunk)\n",
    "    \n",
    "    # Update the minimum and maximum frames with each chunk\n",
    "    minFrame = np.min([minFrame, chunk['frame'].min()])\n",
    "    maxFrame = np.max([maxFrame, chunk['frame'].max()])\n",
    "    \n",
    "    # Rename the columns because trackpy does not accept column names with units\n",
    "    chunk.rename(columns = {'x [nm]' : 'x', 'y [nm]' : 'y'}, inplace = True)\n",
    "    \n",
    "    # It's important to set resetRegions = False here.\n",
    "    # Otherwise, the regions will be overwritten for each new chunk.\n",
    "    dc.iSearch(chunk, resetRegions = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll chunk and loop over the data again, keeping localizations in the previously defined search regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/ipykernel/ipkernel.py:175: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  shell.run_cell(code, store_history=store_history, silent=silent)\n"
     ]
    }
   ],
   "source": [
    "# fids will hold the localizations belonging to fiducials and will grow with each processed chunk\n",
    "fids = pd.DataFrame()\n",
    "\n",
    "# We don't need large chunks anymore, so let's set them to a smaller size (1 million in this case)\n",
    "reader = pd.read_csv(str(filePath.resolve()), chunksize = numRowsToRead)\n",
    "for chunk in reader:\n",
    "    chunk = clean(chunk)\n",
    "    \n",
    "    # Rename the columns because trackpy does not accept column names with units\n",
    "    chunk.rename(columns = {'x [nm]' : 'x', 'y [nm]' : 'y'}, inplace = True)\n",
    "    \n",
    "    # Filter out localizations that are not within all previously-defined search regions.\n",
    "    # Append these to the fiducial Data Frame defined just before the start of the loop.\n",
    "    currentFids = dc.reduceSearchArea(chunk)\n",
    "    fids        = fids.append(currentFids, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect the fiducials within the selected subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 49999: 3 trajectories present\n",
      "3 fiducial(s) detected.\n"
     ]
    }
   ],
   "source": [
    "# Detect the fiducuial trajectories from these localizations\n",
    "dc.detectFiducials(fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the correction curves\n",
    "dc.fitSplines()\n",
    "dc.combineSplines(None, startFrame = minFrame, stopFrame = maxFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were detected fiducials and everything went well, we can now plot the fiducial tracks and the average spline to verify that we have a good drift correction curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc.plotFiducials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the actual drift correction on the data\n",
    "Now that we have the drift curves, we need to perform the actual correction. This is achieved by chunking the same file as before and dynamically writing the corrected data to another file while each chunk is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglass/anaconda3/envs/DataSTORM/lib/python3.5/site-packages/ipykernel/ipkernel.py:175: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  shell.run_cell(code, store_history=store_history, silent=silent)\n"
     ]
    }
   ],
   "source": [
    "reader = pd.read_csv(str(filePath.resolve()), chunksize = numRowsToRead) # We'll read fewer rows this time\n",
    "headerSwitch = True\n",
    "for chunk in reader:\n",
    "    chunk = clean(chunk)\n",
    "    \n",
    "    # Change the column names\n",
    "    chunk.rename(columns = {'x [nm]' : 'x', 'y [nm]' : 'y'}, inplace = True)\n",
    "    \n",
    "    # Remove fiducials from data\n",
    "    chunk = chunk[~((chunk['x'].isin(fids['x']) & (chunk['y'].isin(fids['y']))))]\n",
    "    \n",
    "    # Correct the localizations\n",
    "    chunk = dc.correctLocalizations(chunk)\n",
    "    \n",
    "    # Change the column names back\n",
    "    chunk.rename(columns = {'x'  : 'x [nm]',\n",
    "                            'y'  : 'y [nm]',\n",
    "                            'dx' : 'dx [nm]',\n",
    "                            'dy' : 'dy [nm]'},\n",
    "                 inplace = True)\n",
    "    \n",
    "    # Write the contents to a file, writing the header on the first write only\n",
    "    if headerSwitch:\n",
    "        chunk.to_csv(str(outputFile), mode = 'w', header = True, index = False)\n",
    "        headerSwitch = False\n",
    "    else:\n",
    "        chunk.to_csv(str(outputFile), mode = 'a', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
